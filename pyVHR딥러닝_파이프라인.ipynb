{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j-jxNS7pwI58",
        "fLz_uYURs_Js",
        "6qoETcpstUgj",
        "YEzj_F3-sH58"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1c6ERACmIP8HqaIgE-V18rTSTiWibKmxD",
      "authorship_tag": "ABX9TyP38ylRXBZDUOvLDIoQC9ay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aytekin827/TIL/blob/main/pyVHR%EB%94%A5%EB%9F%AC%EB%8B%9D_%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# < **pyVHR 딥러닝 파이프라인** >\n",
        "---"
      ],
      "metadata": {
        "id": "LI6RY5tarnnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경설정\n",
        "\n"
      ],
      "metadata": {
        "id": "j-jxNS7pwI58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cupy 설치  \n"
      ],
      "metadata": {
        "id": "UWb-JKMazQv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. cuda 버전 확인  \n",
        "2. cupy-cuda11x 삭제 후 재설치  \n",
        "3. `import cupy`\n"
      ],
      "metadata": {
        "id": "B1ynY3Luugtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[코렙 cuda 버전정보 확인]\n",
        "- Torch version:2.0.1+cu118  \n",
        "- cuda version: 11.8  \n",
        "- cudnn version:8700"
      ],
      "metadata": {
        "id": "o7c8oapcze7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdfhtbUQvjp1",
        "outputId": "ce3c1867-f966-471f-cdcc-b4c52358d760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall cupy-cuda11x -Y\n",
        "!pip install cupy-cuda11x"
      ],
      "metadata": {
        "id": "P7XWiBN2sgPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52w2I0oRvEwB",
        "outputId": "38c2b87f-d9b0-46fd-8c15-350cda3761fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/cupy/_environment.py:447: UserWarning: \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  CuPy may not function correctly because multiple CuPy packages are installed\n",
            "  in your environment:\n",
            "\n",
            "    cupy-cuda110, cupy-cuda11x\n",
            "\n",
            "  Follow these steps to resolve this issue:\n",
            "\n",
            "    1. For all packages listed above, run the following command to remove all\n",
            "       existing CuPy installations:\n",
            "\n",
            "         $ pip uninstall <package_name>\n",
            "\n",
            "      If you previously installed CuPy via conda, also run the following:\n",
            "\n",
            "         $ conda uninstall cupy\n",
            "\n",
            "    2. Install the appropriate CuPy package.\n",
            "       Refer to the Installation Guide for detailed instructions.\n",
            "\n",
            "         https://docs.cupy.dev/en/stable/install.html\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  warnings.warn(f'''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mediapipe 설치"
      ],
      "metadata": {
        "id": "PxkiRI88zTVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DieoyykGzU7l",
        "outputId": "188b9698-24e5-440e-9af7-b08b7d94ab9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.3 sounddevice-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 클래스 정의"
      ],
      "metadata": {
        "id": "3AA_YU7Qwfs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pyVHR/extraction/utils"
      ],
      "metadata": {
        "id": "fLz_uYURs_Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import prange, njit\n",
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.signal import welch, butter, filtfilt, iirnotch, freqz\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import iqr, median_abs_deviation\n",
        "\n",
        "class MotionAnalysis():\n",
        "  \"\"\"\n",
        "    Extraction MediaPipe landmarks for motion analysis\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, sig_extractor, winsize, fps, stride=1, landmks=None, Q_notch=10):\n",
        "    self.winsize = winsize\n",
        "    self.fps = fps\n",
        "    # Q factor of notch filter\n",
        "    self.Q_notch = Q_notch\n",
        "    self.stride = stride\n",
        "    # selected MediaPipe landmarks\n",
        "    self.select_landmarks(landmks)\n",
        "    # all MediaPipe landmarks\n",
        "    self.landmarks = sig_extractor.get_landmarks()\n",
        "    # lanmk IDs effectively used within filter\n",
        "    self.landmarks_idx = sig_extractor.ldmks\n",
        "    # shapes of cropped frames\n",
        "    self.shapes = sig_extractor.get_cropped_skin_im_shapes()\n",
        "    # win_landmks: lanmks in a win, win_x: x components, win_y: y componets, times: win times\n",
        "    self.movements_windowing()\n",
        "\n",
        "  def select_landmarks(self, landmks=None):\n",
        "    \"\"\"\n",
        "    Landmark selection from MediaPipe for motion analysis\n",
        "\n",
        "      Args:\n",
        "        landmarks (float32 ndarray): ndarray with shape [num_frames, num_estimators, 2-coords].\n",
        "        shapes (float32 ndarray)   : ndarray with shape [2-coords, num_frames]\n",
        "        wsize (float)              : window size in seconds.\n",
        "        stride (float)             : stride between overlapping windows in seconds.\n",
        "        fps (float)                : frames per seconds.\n",
        "\n",
        "    \"\"\"\n",
        "    # default landmks selected from front to nose\n",
        "    if landmks is None:\n",
        "      self.landmks_selected = [10, 151, 6, 197, 5, 4]\n",
        "    else:\n",
        "       self.landmks_selected = landmks\n",
        "\n",
        "  def movements_windowing(self):\n",
        "    \"\"\"\n",
        "    Calculation of overlapping windows of landmarks coordinates.\n",
        "\n",
        "    Uses:\n",
        "        landmarks (float32 ndarray): ndarray with shape [num_frames, num_estimators, 2-coords].\n",
        "        shapes (float32 ndarray)   : ndarray with shape [2-coords, num_frames]\n",
        "        wsize (float)              : window size in seconds.\n",
        "        stride (float)             : stride between overlapping windows in seconds.\n",
        "        fps (float)                : frames per seconds.\n",
        "\n",
        "    Provides:\n",
        "        A list of ndarray (float32) with shape [wsize, num_estimators, 2-coords]\n",
        "        A list of ndarray (float32) with shape [wsize]\n",
        "        A list of ndarray (float32) with shape [wsize]\n",
        "        and an array (float32) of times in seconds (win centers)\n",
        "    \"\"\"\n",
        "    N = self.landmarks.shape[0]\n",
        "    block_idx, timesLmks = sliding_straded_win_idx(N, self.winsize, self.stride, self.fps)\n",
        "    lmks_xy = []\n",
        "    win_x = []\n",
        "    win_y = []\n",
        "    for e in block_idx:\n",
        "      st_frame = int(e[0])\n",
        "      end_frame = int(e[-1])\n",
        "      coords = np.copy(self.landmarks[st_frame: end_frame+1])\n",
        "      x_coords = np.copy(self.shapes[1][st_frame: end_frame+1])\n",
        "      y_coords = np.copy(self.shapes[0][st_frame: end_frame+1])\n",
        "      lmks_xy.append(coords)\n",
        "      win_x.append(x_coords)\n",
        "      win_y.append(y_coords)\n",
        "    self.win_landmks = np.array(lmks_xy)\n",
        "    self.win_x = np.array(win_x)\n",
        "    self.win_y = np.array(win_y)\n",
        "    self.timesLmks = timesLmks\n",
        "\n",
        "  def get_win_motion_filter_old(self, win):\n",
        "    # loop on landmarks\n",
        "    pos = [self.landmarks_idx.index(i) for i in self.landmks_selected if i in self.landmarks_idx]\n",
        "    X_cords = self.win_landmks[win][:,pos,0]\n",
        "    Y_cords = self.win_landmks[win][:,pos,1]\n",
        "    X_cords = X_cords.T     # shape: (#lanmks, winsize)\n",
        "    Y_cords = Y_cords.T     # shape: (#lanmks, winsize)\n",
        "    Z_cords = np.expand_dims(self.win_x[win], axis=0)\n",
        "\n",
        "    Wx, Hx, Ex = self.mov_notch(X_cords)\n",
        "    Wy, Hy, Ey = self.mov_notch(Y_cords)\n",
        "    Wz, Hz, Ez = self.mov_notch(Z_cords)\n",
        "\n",
        "    return Hx, Hy, Hz, Ex, Ey, Ez\n",
        "\n",
        "  def mov_notch(self, coords):\n",
        "\n",
        "    # coords filtering\n",
        "    b, a = butter(6, Wn=[.65,4.0], fs=self.fps, btype='bandpass')\n",
        "    coords = filtfilt(b, a, coords, axis=1)\n",
        "    F, P = Welch(coords, self.fps)  # shape: (#lanmks, winsize)\n",
        "    P = P.T\n",
        "    pca = PCA(n_components=1)   # PCA\n",
        "    pca.fit(P)\n",
        "    P = pca.fit_transform(P).T\n",
        "    P = P.squeeze()\n",
        "\n",
        "    # energy\n",
        "    P_min = np.min([np.min(P), 0])\n",
        "    En = np.sum(P-P_min)/(P.shape[0])\n",
        "\n",
        "    # movement notch\n",
        "    idx = np.argmax(P)\n",
        "    P_max = P[idx]\n",
        "    F_max = F[idx]\n",
        "    b, a = iirnotch(F_max/60.0, self.Q_notch, self.fps)\n",
        "    W, H = freqz(b, a, worN=1025,  fs=self.fps)\n",
        "    band = np.argwhere((W > 0.65) & (W < 4.0)).flatten()\n",
        "    W_notch = 60.0*W[band]\n",
        "    H_notch = np.abs(H[band])\n",
        "\n",
        "    #plt.plot(F, P)\n",
        "    #plt.show()\n",
        "    #plt.plot(W_notch, H_notch)\n",
        "    #plt.show()\n",
        "\n",
        "    return W_notch, H_notch, En\n",
        "\n",
        "\n",
        "  def get_win_motion_filter(self, win):\n",
        "    # loop on landmarks\n",
        "    pos = [self.landmarks_idx.index(i) for i in self.landmks_selected if i in self.landmarks_idx]\n",
        "    X_cords = self.win_landmks[win][:,pos,0]\n",
        "    Y_cords = self.win_landmks[win][:,pos,1]\n",
        "    X_cords = X_cords.T     # shape: (#lanmks, winsize)\n",
        "    Y_cords = Y_cords.T     # shape: (#lanmks, winsize)\n",
        "    Z_cords = np.expand_dims(self.win_x[win], axis=0)\n",
        "\n",
        "    # filtering\n",
        "    b, a = butter(6, Wn=[.65,4.0], fs=self.fps, btype='bandpass')\n",
        "\n",
        "    #----- X -----\n",
        "    X_cords = filtfilt(b, a, X_cords, axis=1)\n",
        "    _, XPow = Welch(X_cords, self.fps)  # shape: (#lanmks, winsize)\n",
        "    XPow = XPow.T\n",
        "    pcax = PCA(n_components=1)   # PCA\n",
        "    pcax.fit(XPow)\n",
        "    XPow = pcax.fit_transform(XPow).T\n",
        "    XPmov = XPow.squeeze()\n",
        "    # energy\n",
        "    XPmov_min = np.min([np.min(XPmov), 0])\n",
        "    XEnergy = np.sum(XPmov-XPmov_min)/(XPmov.shape[0])\n",
        "    XFmov = 1 - XPmov/np.max(XPmov) # X filter\n",
        "\n",
        "    #-----Y-----\n",
        "    Y_cords = filtfilt(b, a, Y_cords, axis=1)\n",
        "    _, YPow = Welch(Y_cords, self.fps)  # shape: (#lanmks, winsize)\n",
        "    YPow = YPow.T\n",
        "    pcay = PCA(n_components=1)  #PCA\n",
        "    pcay.fit(YPow)\n",
        "    YPow = pcay.fit_transform(YPow).T\n",
        "    YPmov = YPow.squeeze()\n",
        "    # energy\n",
        "    YPmov_min = np.min([np.min(YPmov), 0])\n",
        "    YEnergy = np.sum(YPmov - YPmov_min) / (YPmov.shape[0])\n",
        "    YFmov = 1 - YPmov/np.max(YPmov)  # Y filter\n",
        "\n",
        "    #-----Z-----\n",
        "    Z_cords = filtfilt(b, a, Z_cords, axis=1)\n",
        "    _, ZPow = Welch(Z_cords, self.fps)\n",
        "    ZPow = ZPow.T\n",
        "    ZPmov = ZPow.squeeze()\n",
        "    ZEnergy = np.sum(ZPmov) / (ZPmov.shape[0])\n",
        "    ZFmov = 1 - ZPmov/np.max(ZPmov) # Z filter\n",
        "\n",
        "    return XFmov, YFmov, ZFmov, XEnergy, YEnergy, ZEnergy\n",
        "\n",
        "class MagicLandmarks():\n",
        "    \"\"\"\n",
        "    This class contains usefull lists of landmarks identification numbers.\n",
        "    \"\"\"\n",
        "    high_prio_forehead = [10, 67, 69, 104, 108, 109, 151, 299, 337, 338]\n",
        "    high_prio_nose = [3, 4, 5, 6, 45, 51, 115, 122, 131, 134, 142, 174, 195, 196, 197, 198,\n",
        "                      209, 217, 220, 236, 248, 275, 277, 281, 360, 363, 399, 419, 420, 429, 437, 440, 456]\n",
        "    high_prio_left_cheek = [36, 47, 50, 100, 101, 116, 117,\n",
        "                            118, 119, 123, 126, 147, 187, 203, 205, 206, 207, 216]\n",
        "    high_prio_right_cheek = [266, 280, 329, 330, 346, 347,\n",
        "                             347, 348, 355, 371, 411, 423, 425, 426, 427, 436]\n",
        "\n",
        "    mid_prio_forehead = [8, 9, 21, 68, 103, 251,\n",
        "                         284, 297, 298, 301, 332, 333, 372, 383]\n",
        "    mid_prio_nose = [1, 44, 49, 114, 120, 121, 128, 168, 188, 351, 358, 412]\n",
        "    mid_prio_left_cheek = [34, 111, 137, 156, 177, 192, 213, 227, 234]\n",
        "    mid_prio_right_cheek = [340, 345, 352, 361, 454]\n",
        "    mid_prio_chin = [135, 138, 169, 170, 199, 208, 210, 211,\n",
        "                     214, 262, 288, 416, 428, 430, 431, 432, 433, 434]\n",
        "    mid_prio_mouth = [92, 164, 165, 167, 186, 212, 322, 391, 393, 410]\n",
        "    # more specific areas\n",
        "    forehead_left = [21, 71, 68, 54, 103, 104, 63, 70,\n",
        "                     53, 52, 65, 107, 66, 108, 69, 67, 109, 105]\n",
        "    forehead_center = [10, 151, 9, 8, 107, 336, 285, 55, 8]\n",
        "    forehoead_right = [338, 337, 336, 296, 285, 295, 282,\n",
        "                       334, 293, 301, 251, 298, 333, 299, 297, 332, 284]\n",
        "    eye_right = [283, 300, 368, 353, 264, 372, 454, 340, 448,\n",
        "                 450, 452, 464, 417, 441, 444, 282, 276, 446, 368]\n",
        "    eye_left = [127, 234, 34, 139, 70, 53, 124,\n",
        "                35, 111, 228, 230, 121, 244, 189, 222, 143]\n",
        "    nose = [193, 417, 168, 188, 6, 412, 197, 174, 399, 456,\n",
        "            195, 236, 131, 51, 281, 360, 440, 4, 220, 219, 305]\n",
        "    mounth_up = [186, 92, 167, 393, 322, 410, 287, 39, 269, 61, 164]\n",
        "    mounth_down = [43, 106, 83, 18, 406, 335, 273, 424, 313, 194, 204]\n",
        "    chin = [204, 170, 140, 194, 201, 171, 175,\n",
        "            200, 418, 396, 369, 421, 431, 379, 424]\n",
        "    cheek_left_bottom = [215, 138, 135, 210, 212, 57, 216, 207, 192]\n",
        "    cheek_right_bottom = [435, 427, 416, 364,\n",
        "                          394, 422, 287, 410, 434, 436]\n",
        "    cheek_left_top = [116, 111, 117, 118, 119, 100, 47, 126, 101, 123,\n",
        "                      137, 177, 50, 36, 209, 129, 205, 147, 177, 215, 187, 207, 206, 203]\n",
        "    cheek_right_top = [349, 348, 347, 346, 345, 447, 323,\n",
        "                       280, 352, 330, 371, 358, 423, 426, 425, 427, 411, 376]\n",
        "    # dense zones used for convex hull masks\n",
        "    left_eye = [157,144, 145, 22, 23, 25, 154, 31, 160, 33, 46, 52, 53, 55, 56, 189, 190, 63, 65, 66, 70, 221, 222, 223, 225, 226, 228, 229, 230, 231, 232, 105, 233, 107, 243, 124]\n",
        "    right_eye = [384, 385, 386, 259, 388, 261, 265, 398, 276, 282, 283, 285, 413, 293, 296, 300, 441, 442, 445, 446, 449, 451, 334, 463, 336, 464, 467, 339, 341, 342, 353, 381, 373, 249, 253, 255]\n",
        "    mounth = [391, 393, 11, 269, 270, 271, 287, 164, 165, 37, 167, 40, 43, 181, 313, 314, 186, 57, 315, 61, 321, 73, 76, 335, 83, 85, 90, 106]\n",
        "    # equispaced facial points - mouth and eyes are excluded.\n",
        "    equispaced_facial_points = [2, 3, 4, 5, 6, 8, 9, 10, 18, 21, 32, 35, 36, 43, 46, 47, 48, 50, 54, \\\n",
        "             58, 67, 68, 69, 71, 92, 93, 101, 103, 104, 108, 109, 116, 117, \\\n",
        "             118, 123, 132, 134, 135, 138, 139, 142, 148, 149, 150, 151, 152, 182, 187, 188, 193, 197, 201, 205, 206, 207, \\\n",
        "             210, 211, 212, 216, 234, 248, 251, 262, 265, 266, 273, 277, 278, 280, \\\n",
        "             284, 288, 297, 299, 322, 323, 330, 332, 333, 337, 338, 345, \\\n",
        "             346, 361, 363, 364, 367, 368, 371, 377, 379, 411, 412, 417, 421, 425, 426, 427, 430, 432, 436]\n",
        "\n",
        "def get_magic_landmarks():\n",
        "    \"\"\" returns high_priority and mid_priority list of landmarks identification number \"\"\"\n",
        "    return [*MagicLandmarks.forehead_center, *MagicLandmarks.cheek_left_bottom, *MagicLandmarks.cheek_right_bottom], [*MagicLandmarks.forehoead_right, *MagicLandmarks.forehead_left, *MagicLandmarks.cheek_left_top, *MagicLandmarks.cheek_right_top]\n",
        "\n",
        "@njit(parallel=True)\n",
        "def draw_rects(image, xcenters, ycenters, xsides, ysides, color):\n",
        "    \"\"\"\n",
        "    This method is used to draw N rectangles on a image.\n",
        "    \"\"\"\n",
        "    for idx in prange(len(xcenters)):\n",
        "        leftx = int(xcenters[idx] - xsides[idx]/2)\n",
        "        rightx = int(xcenters[idx] + xsides[idx]/2)\n",
        "        topy = int(ycenters[idx] - ysides[idx]/2)\n",
        "        bottomy = int(ycenters[idx] + ysides[idx]/2)\n",
        "        for x in prange(leftx, rightx):\n",
        "            if topy >= 0 and x >= 0 and x < image.shape[1]:\n",
        "                image[topy, x, 0] = color[0]\n",
        "                image[topy, x, 1] = color[1]\n",
        "                image[topy, x, 2] = color[2]\n",
        "            if bottomy < image.shape[0] and x >= 0 and x < image.shape[1]:\n",
        "                image[bottomy, x, 0] = color[0]\n",
        "                image[bottomy, x, 1] = color[1]\n",
        "                image[bottomy, x, 2] = color[2]\n",
        "        for y in prange(topy, bottomy):\n",
        "            if leftx >= 0 and y >= 0 and y < image.shape[0]:\n",
        "                image[y, leftx, 0] = color[0]\n",
        "                image[y, leftx, 1] = color[1]\n",
        "                image[y, leftx, 2] = color[2]\n",
        "            if rightx < image.shape[1] and y >= 0 and y < image.shape[0]:\n",
        "                image[y, rightx, 0] = color[0]\n",
        "                image[y, rightx, 1] = color[1]\n",
        "                image[y, rightx, 2] = color[2]\n",
        "    return image\n",
        "\n",
        "def sig_windowing(sig, wsize, stride, fps):\n",
        "    \"\"\"\n",
        "    This method is used to divide a RGB signal into overlapping windows.\n",
        "\n",
        "    Args:\n",
        "        sig (float32 ndarray): ndarray with shape [num_frames, num_estimators, rgb_channels].\n",
        "        wsize (float): window size in seconds.\n",
        "        stride (float): stride between overlapping windows in seconds.\n",
        "        fps (float): frames per seconds.\n",
        "\n",
        "    Returns:\n",
        "        A list of ndarray (float32) with shape [num_estimators, rgb_channels, window_frames],\n",
        "        an array (float32) of times in seconds (win centers)\n",
        "    \"\"\"\n",
        "    N = sig.shape[0]\n",
        "    block_idx, timesES = sliding_straded_win_idx(N, wsize, stride, fps)\n",
        "    block_signals = []\n",
        "    for e in block_idx:\n",
        "        st_frame = int(e[0])\n",
        "        end_frame = int(e[-1])\n",
        "        wind_signal = np.copy(sig[st_frame: end_frame+1])\n",
        "        wind_signal = np.swapaxes(wind_signal, 0, 1)\n",
        "        wind_signal = np.swapaxes(wind_signal, 1, 2)\n",
        "        block_signals.append(wind_signal)\n",
        "    return block_signals, timesES\n",
        "\n",
        "    \"\"\"\n",
        "    This method is used to divide a Raw signal into overlapping windows.\n",
        "\n",
        "    Args:\n",
        "        sig (float32 ndarray): ndarray of images with shape [num_frames, rows, columns, rgb_channels].\n",
        "        wsize (float): window size in seconds.\n",
        "        stride (float): stride between overlapping windows in seconds.\n",
        "        fps (float): frames per seconds.\n",
        "\n",
        "    Returns:\n",
        "        windowed signal as a list of length num_windows of float32 ndarray with shape [num_frames, rows, columns, rgb_channels],\n",
        "        and a 1D ndarray of times in seconds,where each one is the center of a window.\n",
        "    \"\"\"\n",
        "    N = raw_signal.shape[0]\n",
        "    block_idx, timesES = sliding_straded_win_idx(N, wsize, stride, fps)\n",
        "    block_signals = []\n",
        "    for e in block_idx:\n",
        "        st_frame = int(e[0])\n",
        "        end_frame = int(e[-1])\n",
        "        wind_signal = np.copy(raw_signal[st_frame: end_frame+1])\n",
        "        # check for zero traces\n",
        "        sum_wind = np.sum(wind_signal, axis=(1,2))\n",
        "        zero_idx = np.argwhere(sum_wind == 0).squeeze()\n",
        "        est_idx = np.ones(wind_signal.shape[0], dtype=bool)\n",
        "        est_idx[zero_idx] = False\n",
        "        # append traces\n",
        "        block_signals.append(wind_signal[est_idx])\n",
        "    return block_signals, timesES\n",
        "\n",
        "def sliding_straded_win_idx(N, wsize, stride, fps):\n",
        "    \"\"\"\n",
        "    This method is used to compute the indices for creating an overlapping windows signal.\n",
        "\n",
        "    Args:\n",
        "        N (int): length of the signal.\n",
        "        wsize (float): window size in seconds.\n",
        "        stride (float): stride between overlapping windows in seconds.\n",
        "        fps (float): frames per seconds.\n",
        "\n",
        "    Returns:\n",
        "        List of ranges, each one contains the indices of a window, and a 1D ndarray of times in seconds, where each one is the center of a window.\n",
        "    \"\"\"\n",
        "    wsize_fr = wsize*fps\n",
        "    stride_fr = stride*fps\n",
        "    idx = []\n",
        "    timesES = []\n",
        "    num_win = int((N-wsize_fr)/stride_fr)+1\n",
        "    s = 0\n",
        "    for i in range(num_win):\n",
        "        idx.append(np.arange(s, s+wsize_fr))\n",
        "        s += stride_fr\n",
        "        timesES.append(wsize/2+stride*i)\n",
        "    return idx, np.array(timesES, dtype=np.float32)\n",
        "\n",
        "def get_fps(videoFileName):\n",
        "    \"\"\"\n",
        "    This method returns the fps of a video file name or path.\n",
        "    \"\"\"\n",
        "    vidcap = cv2.VideoCapture(videoFileName)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    vidcap.release()\n",
        "    return fps\n",
        "\n",
        "def Welch(bvps, fs):\n",
        "  _, n = bvps.shape\n",
        "  if n < 256:\n",
        "      seglength = n\n",
        "      overlap = int(0.6*n)  # fixed overlapping\n",
        "  else:\n",
        "      seglength = 256\n",
        "      overlap = 200\n",
        "  # -- periodogram by Welch\n",
        "\n",
        "  if np.isnan(bvps).any():\n",
        "    print('OK bvp')\n",
        "  F, P = welch(bvps, nperseg=seglength, noverlap=overlap, fs=fs, nfft=2048)\n",
        "  F = F.astype(np.float32)\n",
        "  P = P.astype(np.float32)\n",
        "  # -- freq subband (0.65 Hz - 4.0 Hz)\n",
        "  band = np.argwhere((F > 0.65) & (F < 4.0)).flatten()\n",
        "  Pfreqs = 60*F[band]\n",
        "  Power = P[:, band]\n",
        "  return Pfreqs, Power\n",
        "\n",
        "def extract_frames_yield(videoFileName):\n",
        "    \"\"\"\n",
        "    This method yield the frames of a video file name or path.\n",
        "    \"\"\"\n",
        "    vidcap = cv2.VideoCapture(videoFileName)\n",
        "    success, image = vidcap.read()\n",
        "    while success:\n",
        "        yield image\n",
        "        success, image = vidcap.read()\n",
        "    vidcap.release()\n",
        "\n",
        "def med_mad(x):\n",
        "  MED = np.median(x)\n",
        "  MAD = median_abs_deviation(x)\n",
        "  return MED, MAD\n",
        "\n",
        "def adjust_BMPs(bpmES, bpmES0, bpmES1, wsize, thr=10):\n",
        "  \"\"\"\n",
        "    adjust the final estimate by evaluating whether to replace the chosen value\n",
        "    with the discarded one based on the distance between the current value and the median\n",
        "    Args:\n",
        "        bpmES: the estimates chosen.\n",
        "        bpmES0: the estimates given by the first cluster.\n",
        "        bpmES1: the estimates given by the second cluster.\n",
        "        wsize: thr size of the windows used (in seconds)\n",
        "        thr: a multiplier factor for the MAD.\n",
        "    Returns:\n",
        "        bpmES: possibly redefined bpm estimates.\n",
        "    \"\"\"\n",
        "  T = thr\n",
        "  N = len(bpmES)\n",
        "  bpmES = np.array(bpmES)\n",
        "  bpmES0 = np.array(bpmES0)\n",
        "  bpmES1 = np.array(bpmES1)\n",
        "  for i in range(N):\n",
        "    L = max(0,i-wsize)\n",
        "    R = min(N,i+wsize)\n",
        "    MED, MAD = med_mad(bpmES[L:R])\n",
        "    diff = np.abs(bpmES[i]-MED)      # all diffs with MED\n",
        "    if diff > T+MAD:\n",
        "      if abs(bpmES[i]-bpmES0[i]) < 10e-9 and abs(MED-bpmES1[i]) < T+MAD:\n",
        "        bpmES[i] = bpmES1[i]\n",
        "      elif abs(bpmES[i]-bpmES1[i]) < 10e-9 and abs(MED-bpmES0[i]) < T+MAD:\n",
        "        bpmES[i] = bpmES0[i]\n",
        "      else:\n",
        "        bpmES[i] = MED\n",
        "  return bpmES\n"
      ],
      "metadata": {
        "id": "yJduTJ-Os_28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pyVHR/extraction/sig_extraction_methods"
      ],
      "metadata": {
        "id": "6qoETcpstUgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import njit, prange, float32\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "This module defines classes or methods used for signal extraction.\n",
        "\"\"\"\n",
        "\n",
        "class SignalProcessingParams():\n",
        "    \"\"\"\n",
        "        This class contains usefull parameters used by this module.\n",
        "\n",
        "        RGB_LOW_TH (numpy.int32): RGB low-threshold value.\n",
        "\n",
        "        RGB_HIGH_TH (numpy.int32): RGB high-threshold value.\n",
        "    \"\"\"\n",
        "    RGB_LOW_TH = np.int32(55)\n",
        "    RGB_HIGH_TH = np.int32(200)\n",
        "\n",
        "\n",
        "@njit(['float32[:,:](uint8[:,:,:], int32, int32)', ], parallel=True, fastmath=True, nogil=True)\n",
        "def holistic_mean(im, RGB_LOW_TH, RGB_HIGH_TH):\n",
        "    \"\"\"\n",
        "    This method computes the RGB-Mean Signal excluding 'im' pixels\n",
        "    that are outside the RGB range [RGB_LOW_TH, RGB_HIGH_TH] (extremes are included).\n",
        "\n",
        "    Args:\n",
        "        im (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "        RGB_LOW_TH (numpy.int32): RGB low threshold value.\n",
        "        RGB_HIGH_TH (numpy.int32): RGB high threshold value.\n",
        "\n",
        "    Returns:\n",
        "        RGB-Mean Signal as float32 ndarray with shape [1,3], where 1 is the single estimator,\n",
        "        and 3 are r-mean, g-mean and b-mean.\n",
        "    \"\"\"\n",
        "    mean = np.zeros((1, 3), dtype=np.float32)\n",
        "    mean_r = np.float32(0.0)\n",
        "    mean_g = np.float32(0.0)\n",
        "    mean_b = np.float32(0.0)\n",
        "    num_elems = np.float32(0.0)\n",
        "    for x in prange(im.shape[0]):\n",
        "        for y in prange(im.shape[1]):\n",
        "            if not((im[x, y, 0] <= RGB_LOW_TH and im[x, y, 1] <= RGB_LOW_TH and im[x, y, 2] <= RGB_LOW_TH)\n",
        "                    or (im[x, y, 0] >= RGB_HIGH_TH and im[x, y, 1] >= RGB_HIGH_TH and im[x, y, 2] >= RGB_HIGH_TH)):\n",
        "                mean_r += im[x, y, 0]\n",
        "                mean_g += im[x, y, 1]\n",
        "                mean_b += im[x, y, 2]\n",
        "                num_elems += 1.0\n",
        "    if num_elems > 1.0:\n",
        "        mean[0, 0] = mean_r / num_elems\n",
        "        mean[0, 1] = mean_g / num_elems\n",
        "        mean[0, 2] = mean_b / num_elems\n",
        "    else:\n",
        "        mean[0, 0] = mean_r\n",
        "        mean[0, 1] = mean_g\n",
        "        mean[0, 2] = mean_b\n",
        "    return mean\n",
        "\n",
        "\n",
        "@njit(['float32[:,:](float32[:,:],uint8[:,:,:],float32, int32, int32)', ], parallel=True, fastmath=True, nogil=True)\n",
        "def landmarks_mean(ldmks, im, square, RGB_LOW_TH, RGB_HIGH_TH):\n",
        "    \"\"\"\n",
        "    This method computes the RGB-Mean Signal excluding 'im' pixels\n",
        "    that are outside the RGB range [RGB_LOW_TH, RGB_HIGH_TH] (extremes are included).\n",
        "\n",
        "    Args:\n",
        "        ldmks (float32 ndarray): landmakrs as ndarray with shape [num_landmarks, 5],\n",
        "             where the second dimension contains y-coord, x-coord, r-mean (value is not important), g-mean (value is not important), b-mean (value is not important).\n",
        "        im (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "        square (numpy.float32): side size of square patches.\n",
        "        RGB_LOW_TH (numpy.int32): RGB low threshold value.\n",
        "        RGB_HIGH_TH (numpy.int32): RGB high threshold value.\n",
        "\n",
        "    Returns:\n",
        "        RGB-Mean Signal as float32 ndarray with shape [num_landmarks, 5], where the second dimension contains y-coord, x-coord, r-mean, g-mean, b-mean.\n",
        "    \"\"\"\n",
        "    r_ldmks = ldmks.astype(np.float32)\n",
        "    width = im.shape[1]\n",
        "    height = im.shape[0]\n",
        "    S = math.floor(square/2)\n",
        "    lds_mean = np.zeros((ldmks.shape[0], 3), dtype=np.float32)\n",
        "    num_elems = np.zeros((ldmks.shape[0], ), dtype=np.float32)\n",
        "    for ld_id in prange(0, r_ldmks.shape[0]):\n",
        "        if r_ldmks[ld_id, 0] >= 0.0:\n",
        "            for x in prange(int(r_ldmks[ld_id, 0] - S), int(r_ldmks[ld_id, 0] + S + 1)):\n",
        "                for y in prange(int(r_ldmks[ld_id, 1] - S), int(r_ldmks[ld_id, 1] + S + 1)):\n",
        "                    if x >= 0 and x < height and y >= 0 and y < width:\n",
        "                        if not((im[x, y, 0] <= RGB_LOW_TH and im[x, y, 1] <= RGB_LOW_TH and im[x, y, 2] <= RGB_LOW_TH) or\n",
        "                               (im[x, y, 0] >= RGB_HIGH_TH and im[x, y, 1] >= RGB_HIGH_TH and im[x, y, 2] >= RGB_HIGH_TH)):\n",
        "                            lds_mean[ld_id, 0] += np.float32(im[x, y, 0])\n",
        "                            lds_mean[ld_id, 1] += np.float32(im[x, y, 1])\n",
        "                            lds_mean[ld_id, 2] += np.float32(im[x, y, 2])\n",
        "                            num_elems[ld_id] += 1.0\n",
        "            if num_elems[ld_id] > 1.0:\n",
        "                r_ldmks[ld_id, 2] = lds_mean[ld_id, 0] / num_elems[ld_id]\n",
        "                r_ldmks[ld_id, 3] = lds_mean[ld_id, 1] / num_elems[ld_id]\n",
        "                r_ldmks[ld_id, 4] = lds_mean[ld_id, 2] / num_elems[ld_id]\n",
        "    return r_ldmks\n",
        "\n",
        "\n",
        "@njit(['float32[:,:](float32[:,:],uint8[:,:,:],float32, int32, int32)', ], parallel=True, fastmath=True, nogil=True)\n",
        "def landmarks_median(ldmks, im, square, RGB_LOW_TH, RGB_HIGH_TH):\n",
        "    \"\"\"\n",
        "    This method computes the RGB-Median Signal excluding 'im' pixels\n",
        "    that are outside the RGB range [RGB_LOW_TH, RGB_HIGH_TH] (extremes are included).\n",
        "\n",
        "    Args:\n",
        "        ldmks (float32 ndarray): landmakrs as ndarray with shape [num_landmarks, 5],\n",
        "             where the second dimension contains y-coord, x-coord, r-mean (value is not important), g-mean (value is not important), b-mean (value is not important).\n",
        "        im (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "        square (numpy.float32): side size of square patches.\n",
        "        RGB_LOW_TH (numpy.int32): RGB low threshold value.\n",
        "        RGB_HIGH_TH (numpy.int32): RGB high threshold value.\n",
        "\n",
        "    Returns:\n",
        "        RGB-Median Signal as float32 ndarray with shape [num_landmarks, 5], where the second dimension contains y-coord, x-coord, r-mean, g-mean, b-mean.\n",
        "    \"\"\"\n",
        "    r_ldmks = ldmks.astype(np.float32)\n",
        "    width = float32(im.shape[1])\n",
        "    height = float32(im.shape[0])\n",
        "    S = math.floor(square/2.0)\n",
        "    for ld_id in prange(r_ldmks.shape[0]):\n",
        "        if r_ldmks[ld_id, 0] >= 0.0:\n",
        "            x_s = r_ldmks[ld_id, 0] - S\n",
        "            if x_s < 0.0:\n",
        "                x_s = 0\n",
        "            x_e = r_ldmks[ld_id, 0] + S\n",
        "            if x_e >= height:\n",
        "                x_e = height\n",
        "            y_s = r_ldmks[ld_id, 1] - S\n",
        "            if y_s < 0.0:\n",
        "                y_s = 0\n",
        "            y_e = r_ldmks[ld_id, 1] + S\n",
        "            if y_e >= width:\n",
        "                y_e = width\n",
        "            ar = np.copy(im[int(x_s):int(x_e), int(y_s):int(y_e), :])\n",
        "            f_ar = ar.flatten()\n",
        "            r = ar[:, :, 0].flatten()\n",
        "            g = ar[:, :, 1].flatten()\n",
        "            b = ar[:, :, 2].flatten()\n",
        "            goodidx = np.ones((ar.shape[0]*ar.shape[1],), dtype=np.int32)\n",
        "            targets = np.arange(3, f_ar.shape[0], 3)\n",
        "            for idx in prange(targets.shape[0]):\n",
        "                i = targets[idx]\n",
        "                if ((f_ar[i-2] <= RGB_LOW_TH and f_ar[i-1] <= RGB_LOW_TH and f_ar[i] <= RGB_LOW_TH) or\n",
        "                        (f_ar[i-2] >= RGB_HIGH_TH and f_ar[i-1] >= RGB_HIGH_TH and f_ar[i] >= RGB_HIGH_TH)):\n",
        "                    goodidx[i % 3] = 0\n",
        "            goodidx = np.argwhere(goodidx).flatten()\n",
        "            if goodidx.size < 1 or r.size < 1:\n",
        "                r_ldmks[ld_id, 2] = np.float32(0.0)\n",
        "                r_ldmks[ld_id, 3] = np.float32(0.0)\n",
        "                r_ldmks[ld_id, 4] = np.float32(0.0)\n",
        "            else:\n",
        "                r_ldmks[ld_id, 2] = np.float32(np.median(r[goodidx]))\n",
        "                r_ldmks[ld_id, 3] = np.float32(np.median(g[goodidx]))\n",
        "                r_ldmks[ld_id, 4] = np.float32(np.median(b[goodidx]))\n",
        "    return r_ldmks\n",
        "\n",
        "\n",
        "@njit(['float32[:,:](float32[:,:],uint8[:,:,:],float32[:,:], int32, int32)', ], parallel=True, fastmath=True, nogil=True)\n",
        "def landmarks_mean_custom_rect(ldmks, im, rects, RGB_LOW_TH, RGB_HIGH_TH):\n",
        "    \"\"\"\n",
        "    This method computes the RGB-Mean Signal excluding 'im' pixels\n",
        "    that are outside the RGB range [RGB_LOW_TH, RGB_HIGH_TH] (extremes are included).\n",
        "\n",
        "    Args:\n",
        "        ldmks (float32 ndarray): landmakrs as ndarray with shape [num_landmarks, 5],\n",
        "             where the second dimension contains y-coord, x-coord, r-mean (value is not important), g-mean (value is not important), b-mean (value is not important).\n",
        "        im (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "        rects (float32 ndarray): positive float32 np.ndarray of shape [num_landmarks, 2]. If the list of used landmarks is [1,2,3]\n",
        "            and rects_dim is [[10,20],[12,13],[40,40]] then the landmark number 2 will have a rectangular patch of xy-dimension 12x13.\n",
        "        RGB_LOW_TH (numpy.int32): RGB low threshold value.\n",
        "        RGB_HIGH_TH (numpy.int32): RGB high threshold value.\n",
        "\n",
        "    Returns:\n",
        "        RGB-Mean Signal as float32 ndarray with shape [num_landmarks, 5], where the second dimension contains y-coord, x-coord, r-mean, g-mean, b-mean.\n",
        "    \"\"\"\n",
        "    r_ldmks = ldmks.astype(np.float32)\n",
        "    width = im.shape[1]\n",
        "    height = im.shape[0]\n",
        "    lds_mean = np.zeros((ldmks.shape[0], 3), dtype=np.float32)\n",
        "    num_elems = np.zeros((ldmks.shape[0], ), dtype=np.float32)\n",
        "    for ld_id in prange(0, r_ldmks.shape[0]):\n",
        "        if r_ldmks[ld_id, 0] >= 0:\n",
        "            Sx = math.floor(rects[ld_id, 1]/2)\n",
        "            Sy = math.floor(rects[ld_id, 0]/2)\n",
        "            for x in prange(r_ldmks[ld_id, 0] - Sx, r_ldmks[ld_id, 0] + Sx + 1):\n",
        "                for y in prange(r_ldmks[ld_id, 1] - Sy, r_ldmks[ld_id, 1] + Sy + 1):\n",
        "                    if x >= 0 and x < height and y >= 0 and y < width:\n",
        "                        if not((im[x, y, 0] <= RGB_LOW_TH and im[x, y, 1] <= RGB_LOW_TH and im[x, y, 2] <= RGB_LOW_TH) or\n",
        "                               (im[x, y, 0] >= RGB_HIGH_TH and im[x, y, 1] >= RGB_HIGH_TH and im[x, y, 2] >= RGB_HIGH_TH)):\n",
        "                            lds_mean[ld_id, 0] += im[x, y, 0]\n",
        "                            lds_mean[ld_id, 1] += im[x, y, 1]\n",
        "                            lds_mean[ld_id, 2] += im[x, y, 2]\n",
        "                            num_elems[ld_id] += 1.0\n",
        "            if num_elems[ld_id] > 1.0:\n",
        "                r_ldmks[ld_id, 2] = lds_mean[ld_id, 0] / num_elems[ld_id]\n",
        "                r_ldmks[ld_id, 3] = lds_mean[ld_id, 1] / num_elems[ld_id]\n",
        "                r_ldmks[ld_id, 4] = lds_mean[ld_id, 2] / num_elems[ld_id]\n",
        "    return r_ldmks\n",
        "\n",
        "\n",
        "@njit(['float32[:,:](float32[:,:],uint8[:,:,:],float32[:,:], int32, int32)', ], parallel=True, fastmath=True, nogil=True)\n",
        "def landmarks_median_custom_rect(ldmks, im, rects, RGB_LOW_TH, RGB_HIGH_TH):\n",
        "    \"\"\"\n",
        "    This method computes the RGB-Median Signal excluding 'im' pixels\n",
        "    that are outside the RGB range [RGB_LOW_TH, RGB_HIGH_TH] (extremes are included).\n",
        "\n",
        "    Args:\n",
        "        ldmks (float32 ndarray): landmakrs as ndarray with shape [num_landmarks, 5],\n",
        "             where the second dimension contains y-coord, x-coord, r-mean (value is not important), g-mean (value is not important), b-mean (value is not important).\n",
        "        im (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "        rects (float32 ndarray): positive float32 np.ndarray of shape [num_landmarks, 2]. If the list of used landmarks is [1,2,3]\n",
        "            and rects_dim is [[10,20],[12,13],[40,40]] then the landmark number 2 will have a rectangular patch of xy-dimension 12x13.\n",
        "        RGB_LOW_TH (numpy.int32): RGB low threshold value.\n",
        "        RGB_HIGH_TH (numpy.int32): RGB high threshold value.\n",
        "\n",
        "    Returns:\n",
        "        RGB-Median Signal as float32 ndarray with shape [num_landmarks, 5], where the second dimension contains y-coord, x-coord, r-mean, g-mean, b-mean.\n",
        "    \"\"\"\n",
        "    r_ldmks = ldmks.astype(np.float32)\n",
        "    width = float32(im.shape[1])\n",
        "    height = float32(im.shape[0])\n",
        "    for ld_id in prange(ldmks.shape[0]):\n",
        "        if r_ldmks[ld_id, 0] >= 0.0:\n",
        "            Sx = math.floor(rects[ld_id, 1]/2)\n",
        "            Sy = math.floor(rects[ld_id, 0]/2)\n",
        "            x_s = r_ldmks[ld_id, 0] - Sx\n",
        "            if x_s < 0.0:\n",
        "                x_s = 0\n",
        "            x_e = r_ldmks[ld_id, 0] + Sx\n",
        "            if x_e >= height:\n",
        "                x_e = height\n",
        "            y_s = r_ldmks[ld_id, 1] - Sy\n",
        "            if y_s < 0.0:\n",
        "                y_s = 0\n",
        "            y_e = r_ldmks[ld_id, 1] + Sy\n",
        "            if y_e >= width:\n",
        "                y_e = width\n",
        "            ar = np.copy(im[int(x_s):int(x_e), int(y_s):int(y_e), :])\n",
        "            f_ar = ar.flatten()\n",
        "            r = ar[:, :, 0].flatten()\n",
        "            g = ar[:, :, 1].flatten()\n",
        "            b = ar[:, :, 2].flatten()\n",
        "            goodidx = np.ones((ar.shape[0]*ar.shape[1],), dtype=np.int32)\n",
        "            targets = np.arange(3, f_ar.shape[0], 3)\n",
        "            for idx in prange(targets.shape[0]):\n",
        "                i = targets[idx]\n",
        "                if ((f_ar[i-2] <= RGB_LOW_TH and f_ar[i-1] <= RGB_LOW_TH and f_ar[i] <= RGB_LOW_TH) or\n",
        "                        (f_ar[i-2] >= RGB_HIGH_TH and f_ar[i-1] >= RGB_HIGH_TH and f_ar[i] >= RGB_HIGH_TH)):\n",
        "                    goodidx[i % 3] = 0\n",
        "            goodidx = np.argwhere(goodidx).flatten()\n",
        "            if goodidx.size < 1 or r.size < 1:\n",
        "                r_ldmks[ld_id, 2] = np.int32(0)\n",
        "                r_ldmks[ld_id, 3] = np.int32(0)\n",
        "                r_ldmks[ld_id, 4] = np.int32(0)\n",
        "            else:\n",
        "                r_ldmks[ld_id, 2] = np.int32(np.median(r[goodidx]))\n",
        "                r_ldmks[ld_id, 3] = np.int32(np.median(g[goodidx]))\n",
        "                r_ldmks[ld_id, 4] = np.int32(np.median(b[goodidx]))\n",
        "    return r_ldmks"
      ],
      "metadata": {
        "id": "vOW2OimHtYVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pyVHR/extraction/skin_extraction_methods"
      ],
      "metadata": {
        "id": "7DT7FTn0t8c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiSeNet(nn.Module):\n",
        "    def __init__(self, n_classes, *args, **kwargs):\n",
        "        super(BiSeNet, self).__init__()\n",
        "        self.cp = ContextPath()\n",
        "        ## here self.sp is deleted\n",
        "        self.ffm = FeatureFusionModule(256, 256)\n",
        "        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n",
        "        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n",
        "        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.size()[2:]\n",
        "        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # here return res3b1 feature\n",
        "        feat_sp = feat_res8  # use res3b1 feature to replace spatial path feature\n",
        "        feat_fuse = self.ffm(feat_sp, feat_cp8)\n",
        "\n",
        "        feat_out = self.conv_out(feat_fuse)\n",
        "        feat_out16 = self.conv_out16(feat_cp8)\n",
        "        feat_out32 = self.conv_out32(feat_cp16)\n",
        "\n",
        "        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n",
        "        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n",
        "        return feat_out, feat_out16, feat_out32\n",
        "\n",
        "    def init_weight(self):\n",
        "        for ly in self.children():\n",
        "            if isinstance(ly, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
        "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
        "\n",
        "    def get_params(self):\n",
        "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
        "        for name, child in self.named_children():\n",
        "            child_wd_params, child_nowd_params = child.get_params()\n",
        "            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n",
        "                lr_mul_wd_params += child_wd_params\n",
        "                lr_mul_nowd_params += child_nowd_params\n",
        "            else:\n",
        "                wd_params += child_wd_params\n",
        "                nowd_params += child_nowd_params\n",
        "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params"
      ],
      "metadata": {
        "id": "hhsfwKMSxLX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda, njit, prange\n",
        "import cupy\n",
        "import math\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from numba import prange, njit, cuda\n",
        "# from pyVHR.resources.faceparsing.model import BiSeNet\n",
        "import os\n",
        "# import pyVHR\n",
        "from scipy.spatial import ConvexHull\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "\n",
        "\"\"\"\n",
        "This module defines classes or methods used for skin extraction.\n",
        "\"\"\"\n",
        "\n",
        "### functions and parameters ###\n",
        "\n",
        "class SkinProcessingParams():\n",
        "    \"\"\"\n",
        "        This class contains usefull parameters used by this module.\n",
        "\n",
        "        RGB_LOW_TH (numpy.int32): RGB low-threshold value.\n",
        "\n",
        "        RGB_HIGH_TH (numpy.int32): RGB high-threshold value.\n",
        "    \"\"\"\n",
        "    RGB_LOW_TH = np.int32(55)\n",
        "    RGB_HIGH_TH = np.int32(200)\n",
        "\n",
        "\n",
        "def bbox2_CPU(img):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        img (ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "\n",
        "    Returns:\n",
        "        Four cropping coordinates (row, row, column, column) for removing black borders (RGB [O,O,O]) from img.\n",
        "    \"\"\"\n",
        "    rows = np.any(img, axis=1)\n",
        "    cols = np.any(img, axis=0)\n",
        "    nzrows = np.nonzero(rows)\n",
        "    nzcols = np.nonzero(cols)\n",
        "    if nzrows[0].size == 0 or nzcols[0].size == 0:\n",
        "        return -1, -1, -1, -1\n",
        "    rmin, rmax = np.nonzero(rows)[0][[0, -1]]\n",
        "    cmin, cmax = np.nonzero(cols)[0][[0, -1]]\n",
        "    return rmin, rmax, cmin, cmax\n",
        "\n",
        "\n",
        "def bbox2_GPU(img):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        img (cupy.ndarray): cupy.ndarray with shape [rows, columns, rgb_channels].\n",
        "\n",
        "    Returns:\n",
        "        Four cropping coordinates (row, row, column, column) for removing black borders (RGB [O,O,O]) from img.\n",
        "        The returned variables are on GPU.\n",
        "    \"\"\"\n",
        "    rows = cupy.any(img, axis=1)\n",
        "    cols = cupy.any(img, axis=0)\n",
        "    nzrows = cupy.nonzero(rows)\n",
        "    nzcols = cupy.nonzero(cols)\n",
        "    if nzrows[0].size == 0 or nzcols[0].size == 0:\n",
        "        return -1, -1, -1, -1\n",
        "    rmin, rmax = cupy.nonzero(rows)[0][[0, -1]]\n",
        "    cmin, cmax = cupy.nonzero(cols)[0][[0, -1]]\n",
        "    return rmin, rmax, cmin, cmax\n",
        "\n",
        "\n",
        "### SKIN EXTRACTION CLASSES ###\n",
        "\n",
        "class SkinExtractionFaceParsing():\n",
        "    \"\"\"\n",
        "        This class performs skin extraction on CPU/GPU using Face Parsing.\n",
        "        https://github.com/zllrunning/face-parsing.PyTorch\n",
        "    \"\"\"\n",
        "    def __init__(self, device='CPU'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            device (str): This class can execute code on 'CPU' or 'GPU'.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        n_classes = 19\n",
        "        self.net = BiSeNet(n_classes=n_classes)\n",
        "        if self.device == 'GPU':\n",
        "            self.net.cuda()\n",
        "            self.kernel_cuda_skin_copy_and_filter = kernel_cuda_skin_copy_and_filter()\n",
        "        # save_pth = os.path.dirname(pyVHR.resources.faceparsing.model.__file__) + \"/79999_iter.pth\"\n",
        "        save_pth = '/content/drive/MyDrive/rPPG/pyVHR/resources/faceparsing/79999_iter.pth'\n",
        "        if not os.path.isfile(save_pth):\n",
        "            url = \"https://github.com/phuselab/pyVHR/raw/master/resources/faceparsing/79999_iter.pth\"\n",
        "            print('Downloading faceparsing model...')\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            open(save_pth, 'wb').write(r.content)\n",
        "        self.net.load_state_dict(torch.load(save_pth))\n",
        "        self.net.eval()\n",
        "        self.to_tensor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "    def extract_skin(self, image, ldmks):\n",
        "        \"\"\"\n",
        "        This method extract the skin from an image using Face Parsing.\n",
        "        Landmarks (ldmks) are used to create a facial bounding box for cropping the face; this way\n",
        "        the network used in Face Parsing is more accurate.\n",
        "\n",
        "        Args:\n",
        "            image (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "            ldmks (float32 ndarray): ndarray with shape [num_landmarks, xy_coordinates].\n",
        "\n",
        "        Returns:\n",
        "            Cropped skin-image and non-cropped skin-image; both are uint8 ndarray with shape [rows, columns, rgb_channels].\n",
        "        \"\"\"\n",
        "        # crop with bounding box of ldmks; the network works better if the bounding box is bigger\n",
        "        aviable_ldmks = ldmks[ldmks[:,0] >= 0][:,:2]\n",
        "        min_y, min_x = np.min(aviable_ldmks, axis=0)\n",
        "        max_y, max_x = np.max(aviable_ldmks, axis=0)\n",
        "        min_y *= 0.90\n",
        "        min_x *= 0.90\n",
        "        max_y = max_y * 1.10 if max_y * 1.10 < image.shape[0] else image.shape[0]\n",
        "        max_x = max_x * 1.10 if max_x * 1.10 < image.shape[1] else image.shape[1]\n",
        "        cropped_image = np.copy(image[int(min_y):int(max_y),int(min_x):int(max_x) ,:])\n",
        "        nda_im = np.array(cropped_image)\n",
        "        # prepare the image for the bisenet network\n",
        "        cropped_image = self.to_tensor(cropped_image)\n",
        "        cropped_image = torch.unsqueeze(cropped_image, 0)\n",
        "        cropped_skin_img = self.extraction(cropped_image, nda_im)\n",
        "        # recreate full image using cropped_skin_img\n",
        "        full_skin_image = np.zeros_like(image)\n",
        "        full_skin_image[int(min_y):int(max_y),int(min_x):int(max_x) ,:] = cropped_skin_img\n",
        "        return cropped_skin_img, full_skin_image\n",
        "\n",
        "    def extraction(self, im, nda_im):\n",
        "        \"\"\"\n",
        "        This method performs skin extraction using Face Parsing.\n",
        "\n",
        "        Args:\n",
        "            im (torch.Tensor): torch.Tensor with size [rows, columns, rgb_channels]\n",
        "            nda_im (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "\n",
        "        Returns:\n",
        "            skin-image as uint8 ndarray with shape [rows, columns, rgb_channels].\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            if self.device == 'CPU':\n",
        "                ### bisenet skin detection ###\n",
        "                out = self.net(im)[0]\n",
        "                ### gpu cuda skin copy ###\n",
        "                parsing = out.squeeze(0).argmax(0).numpy()\n",
        "                parsing = parsing.astype(np.int32)\n",
        "                nda_im = nda_im.astype(np.uint8)\n",
        "                # kernel skin copy\n",
        "                return kernel_skin_copy_and_filter(nda_im, parsing, np.int32(SkinProcessingParams.RGB_LOW_TH), np.int32(SkinProcessingParams.RGB_HIGH_TH))\n",
        "            else:\n",
        "                ### bisenet skin detection ###\n",
        "                im = im.cuda()\n",
        "                out = self.net(im)[0]\n",
        "                ### gpu cuda skin copy ###\n",
        "                dev_parsing = out.squeeze(0).argmax(0).type(torch.int32)\n",
        "                dev_nda_im = cuda.to_device(nda_im)\n",
        "                dev_new_im = cuda.to_device(np.zeros_like(nda_im))\n",
        "                low_high_f = cuda.to_device(\n",
        "                    np.array([SkinProcessingParams.RGB_LOW_TH, SkinProcessingParams.RGB_HIGH_TH], dtype=np.int32))\n",
        "                # define number of blocks and threads per block\n",
        "                threadsperblock = (16, 16)\n",
        "                blockspergrid_x = math.ceil(nda_im.shape[0] / threadsperblock[0])\n",
        "                blockspergrid_y = math.ceil(nda_im.shape[1] / threadsperblock[1])\n",
        "                blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "                # kernel invoke\n",
        "                self.kernel_cuda_skin_copy_and_filter[blockspergrid, threadsperblock](\n",
        "                    dev_nda_im, dev_parsing, dev_new_im, low_high_f)\n",
        "                # copy to CPU result\n",
        "                newimg = dev_new_im.copy_to_host()\n",
        "                # free memory\n",
        "                im = None\n",
        "                out = None\n",
        "                dev_nda_im = None\n",
        "                dev_parsing = None\n",
        "                dev_new_im = None\n",
        "                return newimg\n",
        "\n",
        "# SkinExtractionFaceParsing class kernels #\n",
        "def kernel_cuda_skin_copy_and_filter():\n",
        "    \"\"\"\n",
        "    Return a Numba cuda.jit kernel defined as:\n",
        "\n",
        "    @cuda.jit('void(uint8[:,:,:], int32[:,:], uint8[:,:,:], int32[:])')\n",
        "    def __kernel_cuda_skin_copy_and_filter(orig, pars, new, low_high_filter):\n",
        "        ''\n",
        "        This method removes pixels from the image 'orig' that are not skin, or\n",
        "        that are outside the RGB range [low_high_filter[0], low_high_filter[1]] (extremes are included).\n",
        "        ''\n",
        "\n",
        "    This method is important for users who do not use a GPU, beacause they can't compile @cuda.jit.\n",
        "    \"\"\"\n",
        "\n",
        "    @cuda.jit('void(uint8[:,:,:], int32[:,:], uint8[:,:,:], int32[:])')\n",
        "    def __kernel_cuda_skin_copy_and_filter(orig, pars, new, low_high_filter):\n",
        "        \"\"\"\n",
        "        This method removes pixels from the image 'orig' that are not skin, or\n",
        "        that are outside the RGB range [low_high_filter[0], low_high_filter[1]] (extremes are included).\n",
        "        \"\"\"\n",
        "        x, y = cuda.grid(2)\n",
        "        if x < orig.shape[0] and y < orig.shape[1]:\n",
        "            # skin class = 1, nose = 10\n",
        "            if pars[x, y] == 1 or pars[x, y] == 10:\n",
        "                if not ((orig[x, y, 0] <= low_high_filter[0] and orig[x, y, 1] <= low_high_filter[0] and orig[x, y, 2] <= low_high_filter[0]) or\n",
        "                        (orig[x, y, 0] >= low_high_filter[1] and orig[x, y, 1] >= low_high_filter[1] and orig[x, y, 2] >= low_high_filter[1])):\n",
        "                    new[x, y, 0] = orig[x, y, 0]\n",
        "                    new[x, y, 1] = orig[x, y, 1]\n",
        "                    new[x, y, 2] = orig[x, y, 2]\n",
        "\n",
        "    return __kernel_cuda_skin_copy_and_filter\n",
        "\n",
        "\n",
        "@njit('uint8[:,:,:](uint8[:,:,:], int32[:,:], int32, int32)', parallel=True, nogil=True)\n",
        "def kernel_skin_copy_and_filter(orig, pars, RGB_LOW_TH, RGB_HIGH_TH):\n",
        "    \"\"\"\n",
        "    This method removes pixels from the image 'orig' that are not skin, or\n",
        "    that are outside the RGB range [RGB_LOW_TH, RGB_HIGH_TH] (extremes are included).\n",
        "    \"\"\"\n",
        "    new = np.zeros_like(orig)\n",
        "    for x in prange(orig.shape[0]):\n",
        "        for y in prange(orig.shape[1]):\n",
        "            # skin class = 1, nose = 10\n",
        "            if pars[x, y] == 1 or pars[x, y] == 10:\n",
        "                if not ((orig[x, y, 0] <= RGB_LOW_TH and orig[x, y, 1] <= RGB_LOW_TH and orig[x, y, 2] <= RGB_LOW_TH) or\n",
        "                        (orig[x, y, 0] >= RGB_HIGH_TH and orig[x, y, 1] >= RGB_HIGH_TH and orig[x, y, 2] >= RGB_HIGH_TH)):\n",
        "                    new[x, y, 0] = orig[x, y, 0]\n",
        "                    new[x, y, 1] = orig[x, y, 1]\n",
        "                    new[x, y, 2] = orig[x, y, 2]\n",
        "    return new\n",
        "\n",
        "class SkinExtractionConvexHull:\n",
        "    \"\"\"\n",
        "        This class performs skin extraction on CPU/GPU using a Convex Hull segmentation obtained from facial landmarks.\n",
        "    \"\"\"\n",
        "    def __init__(self,device='CPU'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            device (str): This class can execute code on 'CPU' or 'GPU'.\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "\n",
        "    def extract_skin(self,image, ldmks):\n",
        "        \"\"\"\n",
        "        This method extract the skin from an image using Convex Hull segmentation.\n",
        "\n",
        "        Args:\n",
        "            image (uint8 ndarray): ndarray with shape [rows, columns, rgb_channels].\n",
        "            ldmks (float32 ndarray): landmarks used to create the Convex Hull; ldmks is a ndarray with shape [num_landmarks, xy_coordinates].\n",
        "\n",
        "        Returns:\n",
        "            Cropped skin-image and non-cropped skin-image; both are uint8 ndarray with shape [rows, columns, rgb_channels].\n",
        "        \"\"\"\n",
        "        from pyVHR.extraction.sig_processing import MagicLandmarks\n",
        "        aviable_ldmks = ldmks[ldmks[:,0] >= 0][:,:2]\n",
        "        # face_mask convex hull\n",
        "        hull = ConvexHull(aviable_ldmks)\n",
        "        verts = [(aviable_ldmks[v,0], aviable_ldmks[v,1]) for v in hull.vertices]\n",
        "        img = Image.new('L', image.shape[:2], 0)\n",
        "        ImageDraw.Draw(img).polygon(verts, outline=1, fill=1)\n",
        "        mask = np.array(img)\n",
        "        mask = np.expand_dims(mask,axis=0).T\n",
        "\n",
        "        # left eye convex hull\n",
        "        left_eye_ldmks = ldmks[MagicLandmarks.left_eye]\n",
        "        aviable_ldmks = left_eye_ldmks[left_eye_ldmks[:,0] >= 0][:,:2]\n",
        "        if len(aviable_ldmks) > 3:\n",
        "            hull = ConvexHull(aviable_ldmks)\n",
        "            verts = [(aviable_ldmks[v,0], aviable_ldmks[v,1]) for v in hull.vertices]\n",
        "            img = Image.new('L', image.shape[:2], 0)\n",
        "            ImageDraw.Draw(img).polygon(verts, outline=1, fill=1)\n",
        "            left_eye_mask = np.array(img)\n",
        "            left_eye_mask = np.expand_dims(left_eye_mask,axis=0).T\n",
        "        else:\n",
        "            left_eye_mask = np.ones((image.shape[0], image.shape[1],1),dtype=np.uint8)\n",
        "\n",
        "        # right eye convex hull\n",
        "        right_eye_ldmks = ldmks[MagicLandmarks.right_eye]\n",
        "        aviable_ldmks = right_eye_ldmks[right_eye_ldmks[:,0] >= 0][:,:2]\n",
        "        if len(aviable_ldmks) > 3:\n",
        "            hull = ConvexHull(aviable_ldmks)\n",
        "            verts = [(aviable_ldmks[v,0], aviable_ldmks[v,1]) for v in hull.vertices]\n",
        "            img = Image.new('L', image.shape[:2], 0)\n",
        "            ImageDraw.Draw(img).polygon(verts, outline=1, fill=1)\n",
        "            right_eye_mask = np.array(img)\n",
        "            right_eye_mask = np.expand_dims(right_eye_mask,axis=0).T\n",
        "        else:\n",
        "            right_eye_mask = np.ones((image.shape[0], image.shape[1],1),dtype=np.uint8)\n",
        "\n",
        "        # mounth convex hull\n",
        "        mounth_ldmks = ldmks[MagicLandmarks.mounth]\n",
        "        aviable_ldmks = mounth_ldmks[mounth_ldmks[:,0] >= 0][:,:2]\n",
        "        if len(aviable_ldmks) > 3:\n",
        "            hull = ConvexHull(aviable_ldmks)\n",
        "            verts = [(aviable_ldmks[v,0], aviable_ldmks[v,1]) for v in hull.vertices]\n",
        "            img = Image.new('L', image.shape[:2], 0)\n",
        "            ImageDraw.Draw(img).polygon(verts, outline=1, fill=1)\n",
        "            mounth_mask = np.array(img)\n",
        "            mounth_mask = np.expand_dims(mounth_mask,axis=0).T\n",
        "        else:\n",
        "            mounth_mask = np.ones((image.shape[0], image.shape[1],1),dtype=np.uint8)\n",
        "\n",
        "        # apply masks and crop\n",
        "        if self.device == 'GPU':\n",
        "            image = cupy.asarray(image)\n",
        "            mask = cupy.asarray(mask)\n",
        "            left_eye_mask = cupy.asarray(left_eye_mask)\n",
        "            right_eye_mask = cupy.asarray(right_eye_mask)\n",
        "            mounth_mask = cupy.asarray(mounth_mask)\n",
        "        skin_image = image * mask * (1-left_eye_mask) * (1-right_eye_mask) * (1-mounth_mask)\n",
        "\n",
        "        if self.device == 'GPU':\n",
        "            rmin, rmax, cmin, cmax = bbox2_GPU(skin_image)\n",
        "        else:\n",
        "            rmin, rmax, cmin, cmax = bbox2_CPU(skin_image)\n",
        "\n",
        "        cropped_skin_im = skin_image\n",
        "        if rmin >= 0 and rmax >= 0 and cmin >= 0 and cmax >= 0 and rmax-rmin >= 0 and cmax-cmin >= 0:\n",
        "            cropped_skin_im = skin_image[int(rmin):int(rmax), int(cmin):int(cmax)]\n",
        "\n",
        "        if self.device == 'GPU':\n",
        "            cropped_skin_im = cupy.asnumpy(cropped_skin_im)\n",
        "            skin_image = cupy.asnumpy(skin_image)\n",
        "\n",
        "        return cropped_skin_im, skin_image"
      ],
      "metadata": {
        "id": "K2j8FJL8uEzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pyVHR/extraction/sig_processing"
      ],
      "metadata": {
        "id": "z12tuBwFy6Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "# from pyVHR.extraction.utils import *\n",
        "# from pyVHR.extraction.skin_extraction_methods import *\n",
        "# from pyVHR.extraction.sig_extraction_methods import *\n",
        "# from pyVHR.utils.cuda_utils import *\n",
        "\n",
        "\"\"\"\n",
        "This module defines classes or methods used for Signal extraction and processing.\n",
        "\"\"\"\n",
        "\n",
        "class SignalProcessing():\n",
        "    \"\"\"\n",
        "        This class performs offline signal extraction with different methods:\n",
        "\n",
        "        - holistic.\n",
        "\n",
        "        - squared / rectangular patches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Common parameters #\n",
        "        self.tot_frames = None\n",
        "        self.visualize_skin_collection = []\n",
        "        self.skin_extractor = SkinExtractionConvexHull('CPU')\n",
        "        # Patches parameters #\n",
        "        high_prio_ldmk_id, mid_prio_ldmk_id = get_magic_landmarks()\n",
        "        self.ldmks = high_prio_ldmk_id + mid_prio_ldmk_id\n",
        "        self.square = None\n",
        "        self.rects = None\n",
        "        self.visualize_skin = False\n",
        "        self.visualize_landmarks = False\n",
        "        self.visualize_landmarks_number = False\n",
        "        self.visualize_patch = False\n",
        "        self.font_size = 0.3\n",
        "        self.font_color = (255, 0, 0, 255)\n",
        "        self.visualize_skin_collection = []\n",
        "        self.visualize_landmarks_collection = []\n",
        "\n",
        "    def choose_cuda_device(self, n):\n",
        "        \"\"\"\n",
        "        Choose a CUDA device.\n",
        "\n",
        "        Args:\n",
        "            n (int): number of a CUDA device.\n",
        "\n",
        "        \"\"\"\n",
        "        select_cuda_device(n)\n",
        "\n",
        "    def display_cuda_device(self):\n",
        "        \"\"\"\n",
        "        Display your CUDA devices.\n",
        "        \"\"\"\n",
        "        cuda_info()\n",
        "\n",
        "    def set_total_frames(self, n):\n",
        "        \"\"\"\n",
        "        Set the total frames to be processed; if you want to process all the possible frames use n = 0.\n",
        "\n",
        "        Args:\n",
        "            n (int): number of frames to be processed.\n",
        "\n",
        "        \"\"\"\n",
        "        if n < 0:\n",
        "            print(\"[ERROR] n must be a positive number!\")\n",
        "        self.tot_frames = int(n)\n",
        "\n",
        "    def set_skin_extractor(self, extractor):\n",
        "        \"\"\"\n",
        "        Set the skin extractor that will be used for skin extraction.\n",
        "\n",
        "        Args:\n",
        "            extractor: instance of a skin_extraction class (see :py:mod:`pyVHR.extraction.skin_extraction_methods`).\n",
        "\n",
        "        \"\"\"\n",
        "        self.skin_extractor = extractor\n",
        "\n",
        "    def set_visualize_skin_and_landmarks(self, visualize_skin=False, visualize_landmarks=False, visualize_landmarks_number=False, visualize_patch=False):\n",
        "        \"\"\"\n",
        "        Set visualization parameters. You can retrieve visualization output with the\n",
        "        methods :py:meth:`pyVHR.extraction.sig_processing.SignalProcessing.get_visualize_skin`\n",
        "        and :py:meth:`pyVHR.extraction.sig_processing.SignalProcessing.get_visualize_patches`\n",
        "\n",
        "        Args:\n",
        "            visualize_skin (bool): The skin and the patches will be visualized.\n",
        "            visualize_landmarks (bool): The landmarks (centers of patches) will be visualized.\n",
        "            visualize_landmarks_number (bool): The landmarks number will be visualized.\n",
        "            visualize_patch (bool): The patches outline will be visualized.\n",
        "\n",
        "        \"\"\"\n",
        "        self.visualize_skin = visualize_skin\n",
        "        self.visualize_landmarks = visualize_landmarks\n",
        "        self.visualize_landmarks_number = visualize_landmarks_number\n",
        "        self.visualize_patch = visualize_patch\n",
        "\n",
        "    def get_visualize_skin(self):\n",
        "        \"\"\"\n",
        "        Get the skin images produced by the last processing. Remember to\n",
        "        set :py:meth:`pyVHR.extraction.sig_processing.SignalProcessing.set_visualize_skin_and_landmarks`\n",
        "        correctly.\n",
        "\n",
        "        Returns:\n",
        "            list of ndarray: list of cv2 images; each image is a ndarray with shape [rows, columns, rgb_channels].\n",
        "        \"\"\"\n",
        "        return self.visualize_skin_collection\n",
        "\n",
        "    def get_visualize_patches(self):\n",
        "        \"\"\"\n",
        "        Get the 'skin+patches' images produced by the last processing. Remember to\n",
        "        set :py:meth:`pyVHR.extraction.sig_processing.SignalProcessing.set_visualize_skin_and_landmarks`\n",
        "        correctly.\n",
        "\n",
        "        Returns:\n",
        "            list of ndarray: list of cv2 images; each image is a ndarray with shape [rows, columns, rgb_channels].\n",
        "        \"\"\"\n",
        "        return self.visualize_landmarks_collection\n",
        "\n",
        "    def extract_raw(self, videoFileName):\n",
        "        \"\"\"\n",
        "        Extracts raw frames from video.\n",
        "\n",
        "        Args:\n",
        "            videoFileName (str): video file name or path.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: raw frames with shape [num_frames, height, width, rgb_channels].\n",
        "        \"\"\"\n",
        "\n",
        "        frames = []\n",
        "        for frame in extract_frames_yield(videoFileName):\n",
        "                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))   # convert to RGB\n",
        "\n",
        "        return np.array(frames)\n",
        "\n",
        "    ### HOLISTIC METHODS ###\n",
        "\n",
        "    def extract_raw_holistic(self, videoFileName):\n",
        "        \"\"\"\n",
        "        Locates the skin pixels in each frame. This method is intended for rPPG methods that use raw video signal.\n",
        "\n",
        "        Args:\n",
        "            videoFileName (str): video file name or path.\n",
        "\n",
        "        Returns:\n",
        "            float32 ndarray: raw signal as float32 ndarray with shape [num_frames, rows, columns, rgb_channels].\n",
        "        \"\"\"\n",
        "\n",
        "        skin_ex = self.skin_extractor\n",
        "\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_face_mesh = mp.solutions.face_mesh\n",
        "        PRESENCE_THRESHOLD = 0.5\n",
        "        VISIBILITY_THRESHOLD = 0.5\n",
        "\n",
        "        sig = []\n",
        "        processed_frames_count = 0\n",
        "\n",
        "        with mp_face_mesh.FaceMesh(\n",
        "                max_num_faces=1,\n",
        "                min_detection_confidence=0.5,\n",
        "                min_tracking_confidence=0.5) as face_mesh:\n",
        "            for frame in extract_frames_yield(videoFileName):\n",
        "                # convert the BGR image to RGB.\n",
        "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                processed_frames_count += 1\n",
        "                width = image.shape[1]\n",
        "                height = image.shape[0]\n",
        "                # [landmarks, info], with info->x_center ,y_center, r, g, b\n",
        "                ldmks = np.zeros((468, 5), dtype=np.float32)\n",
        "                ldmks[:, 0] = -1.0\n",
        "                ldmks[:, 1] = -1.0\n",
        "                ### face landmarks ###\n",
        "                results = face_mesh.process(image)\n",
        "                if results.multi_face_landmarks:\n",
        "                    face_landmarks = results.multi_face_landmarks[0]\n",
        "                    landmarks = [l for l in face_landmarks.landmark]\n",
        "                    for idx in range(len(landmarks)):\n",
        "                        landmark = landmarks[idx]\n",
        "                        if not ((landmark.HasField('visibility') and landmark.visibility < VISIBILITY_THRESHOLD)\n",
        "                                or (landmark.HasField('presence') and landmark.presence < PRESENCE_THRESHOLD)):\n",
        "                            coords = mp_drawing._normalized_to_pixel_coordinates(\n",
        "                                landmark.x, landmark.y, width, height)\n",
        "                            if coords:\n",
        "                                ldmks[idx, 0] = coords[1]\n",
        "                                ldmks[idx, 1] = coords[0]\n",
        "                    ### skin extraction ###\n",
        "                    cropped_skin_im, full_skin_im = skin_ex.extract_skin(\n",
        "                        image, ldmks)\n",
        "                else:\n",
        "                    cropped_skin_im = np.zeros_like(image)\n",
        "                    full_skin_im = np.zeros_like(image)\n",
        "                if self.visualize_skin == True:\n",
        "                    self.visualize_skin_collection.append(full_skin_im)\n",
        "                ### sig computing ###\n",
        "                sig.append(full_skin_im)\n",
        "                ### loop break ###\n",
        "                if self.tot_frames is not None and self.tot_frames > 0 and processed_frames_count >= self.tot_frames:\n",
        "                    break\n",
        "        sig = np.array(sig, dtype=np.float32)\n",
        "        return sig\n",
        "\n",
        "    def extract_holistic(self, videoFileName):\n",
        "        \"\"\"\n",
        "        This method compute the RGB-mean signal using the whole skin (holistic);\n",
        "\n",
        "        Args:\n",
        "            videoFileName (str): video file name or path.\n",
        "\n",
        "        Returns:\n",
        "            float32 ndarray: RGB signal as ndarray with shape [num_frames, 1, rgb_channels]. The second dimension is 1 because\n",
        "            the whole skin is considered as one estimators.\n",
        "        \"\"\"\n",
        "        self.visualize_skin_collection = []\n",
        "\n",
        "        skin_ex = self.skin_extractor\n",
        "\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_face_mesh = mp.solutions.face_mesh\n",
        "        PRESENCE_THRESHOLD = 0.5\n",
        "        VISIBILITY_THRESHOLD = 0.5\n",
        "\n",
        "        sig = []\n",
        "        processed_frames_count = 0\n",
        "\n",
        "        with mp_face_mesh.FaceMesh(\n",
        "                max_num_faces=1,\n",
        "                min_detection_confidence=0.5,\n",
        "                min_tracking_confidence=0.5) as face_mesh:\n",
        "            for frame in extract_frames_yield(videoFileName):\n",
        "                # convert the BGR image to RGB.\n",
        "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                processed_frames_count += 1\n",
        "                width = image.shape[1]\n",
        "                height = image.shape[0]\n",
        "                # [landmarks, info], with info->x_center ,y_center, r, g, b\n",
        "                ldmks = np.zeros((468, 5), dtype=np.float32)\n",
        "                ldmks[:, 0] = -1.0\n",
        "                ldmks[:, 1] = -1.0\n",
        "                ### face landmarks ###\n",
        "                results = face_mesh.process(image)\n",
        "                if results.multi_face_landmarks:\n",
        "                    face_landmarks = results.multi_face_landmarks[0]\n",
        "                    landmarks = [l for l in face_landmarks.landmark]\n",
        "                    for idx in range(len(landmarks)):\n",
        "                        landmark = landmarks[idx]\n",
        "                        if not ((landmark.HasField('visibility') and landmark.visibility < VISIBILITY_THRESHOLD)\n",
        "                                or (landmark.HasField('presence') and landmark.presence < PRESENCE_THRESHOLD)):\n",
        "                            coords = mp_drawing._normalized_to_pixel_coordinates(\n",
        "                                landmark.x, landmark.y, width, height)\n",
        "                            if coords:\n",
        "                                ldmks[idx, 0] = coords[1]\n",
        "                                ldmks[idx, 1] = coords[0]\n",
        "                    ### skin extraction ###\n",
        "                    cropped_skin_im, full_skin_im = skin_ex.extract_skin(\n",
        "                        image, ldmks)\n",
        "                else:\n",
        "                    cropped_skin_im = np.zeros_like(image)\n",
        "                    full_skin_im = np.zeros_like(image)\n",
        "                if self.visualize_skin == True:\n",
        "                    self.visualize_skin_collection.append(full_skin_im)\n",
        "                ### sig computing ###\n",
        "                sig.append(holistic_mean(\n",
        "                    cropped_skin_im, np.int32(SignalProcessingParams.RGB_LOW_TH), np.int32(SignalProcessingParams.RGB_HIGH_TH)))\n",
        "                ### loop break ###\n",
        "                if self.tot_frames is not None and self.tot_frames > 0 and processed_frames_count >= self.tot_frames:\n",
        "                    break\n",
        "        sig = np.array(sig, dtype=np.float32)\n",
        "        return sig\n",
        "\n",
        "    ### PATCHES METHODS ###\n",
        "\n",
        "    def set_landmarks(self, landmarks_list):\n",
        "        \"\"\"\n",
        "        Set the patches centers (landmarks) that will be used for signal processing. There are 468 facial points you can\n",
        "        choose; for visualizing their identification number please use :py:meth:`pyVHR.plot.visualize.visualize_landmarks_list`.\n",
        "\n",
        "        Args:\n",
        "            landmarks_list (list): list of positive integers between 0 and 467 that identify patches centers (landmarks).\n",
        "        \"\"\"\n",
        "        if not isinstance(landmarks_list, list):\n",
        "            print(\"[ERROR] landmarks_set must be a list!\")\n",
        "            return\n",
        "        self.ldmks = landmarks_list\n",
        "\n",
        "    def set_square_patches_side(self, square_side):\n",
        "        \"\"\"\n",
        "        Set the dimension of the square patches that will be used for signal processing. There are 468 facial points you can\n",
        "        choose; for visualizing their identification number please use :py:meth:`pyVHR.plot.visualize.visualize_landmarks_list`.\n",
        "\n",
        "        Args:\n",
        "            square_side (float): positive float that defines the length of the square patches.\n",
        "        \"\"\"\n",
        "        if not isinstance(square_side, float) or square_side <= 0.0:\n",
        "            print(\"[ERROR] square_side must be a positive float!\")\n",
        "            return\n",
        "        self.square = square_side\n",
        "\n",
        "    def set_rect_patches_sides(self, rects_dim):\n",
        "        \"\"\"\n",
        "        Set the dimension of each rectangular patch. There are 468 facial points you can\n",
        "        choose; for visualizing their identification number please use :py:meth:`pyVHR.plot.visualize.visualize_landmarks_list`.\n",
        "\n",
        "        Args:\n",
        "            rects_dim (float32 ndarray): positive float32 np.ndarray of shape [num_landmarks, 2]. If the list of used landmarks is [1,2,3]\n",
        "                and rects_dim is [[10,20],[12,13],[40,40]] then the landmark number 2 will have a rectangular patch of xy-dimension 12x13.\n",
        "        \"\"\"\n",
        "        if type(rects_dim) != type(np.array([])):\n",
        "            print(\"[ERROR] rects_dim must be an np.ndarray!\")\n",
        "            return\n",
        "        if rects_dim.shape[0] != len(self.ldmks) and rects_dim.shape[1] != 2:\n",
        "            print(\"[ERROR] incorrect rects_dim shape!\")\n",
        "            return\n",
        "        self.rects = rects_dim\n",
        "\n",
        "    def extract_patches(self, videoFileName, region_type, sig_extraction_method):\n",
        "        \"\"\"\n",
        "        This method compute the RGB-mean signal using specific skin regions (patches).\n",
        "\n",
        "        Args:\n",
        "            videoFileName (str): video file name or path.\n",
        "            region_type (str): patches types can be  \"squares\" or \"rects\".\n",
        "            sig_extraction_method (str): RGB signal can be computed with \"mean\" or \"median\". We recommend to use mean.\n",
        "\n",
        "        Returns:\n",
        "            float32 ndarray: RGB signal as ndarray with shape [num_frames, num_patches, rgb_channels].\n",
        "        \"\"\"\n",
        "        if self.square is None and self.rects is None:\n",
        "            print(\n",
        "                \"[ERROR] Use set_landmarks_squares or set_landmarkds_rects before calling this function!\")\n",
        "            return None\n",
        "        if region_type != \"squares\" and region_type != \"rects\":\n",
        "            print(\"[ERROR] Invalid landmarks region type!\")\n",
        "            return None\n",
        "        if sig_extraction_method != \"mean\" and sig_extraction_method != \"median\":\n",
        "            print(\"[ERROR] Invalid signal extraction method!\")\n",
        "            return None\n",
        "\n",
        "        ldmks_regions = None\n",
        "        if region_type == \"squares\":\n",
        "            ldmks_regions = np.float32(self.square)\n",
        "        elif region_type == \"rects\":\n",
        "            ldmks_regions = np.float32(self.rects)\n",
        "\n",
        "        sig_ext_met = None\n",
        "        if sig_extraction_method == \"mean\":\n",
        "            if region_type == \"squares\":\n",
        "                sig_ext_met = landmarks_mean\n",
        "            elif region_type == \"rects\":\n",
        "                sig_ext_met = landmarks_mean_custom_rect\n",
        "        elif sig_extraction_method == \"median\":\n",
        "            if region_type == \"squares\":\n",
        "                sig_ext_met = landmarks_median\n",
        "            elif region_type == \"rects\":\n",
        "                sig_ext_met = landmarks_median_custom_rect\n",
        "\n",
        "        self.visualize_skin_collection = []\n",
        "        self.visualize_landmarks_collection = []\n",
        "\n",
        "        skin_ex = self.skin_extractor\n",
        "\n",
        "        mp_drawing = mp.solutions.drawing_utils\n",
        "        mp_face_mesh = mp.solutions.face_mesh\n",
        "        PRESENCE_THRESHOLD = 0.5\n",
        "        VISIBILITY_THRESHOLD = 0.5\n",
        "\n",
        "        sig = []\n",
        "        processed_frames_count = 0\n",
        "        self.patch_landmarks = []\n",
        "        self.cropped_skin_im_shapes = [[], []]\n",
        "        with mp_face_mesh.FaceMesh(\n",
        "                max_num_faces=1,\n",
        "                min_detection_confidence=0.5,\n",
        "                min_tracking_confidence=0.5) as face_mesh:\n",
        "            for frame in extract_frames_yield(videoFileName):\n",
        "                # convert the BGR image to RGB.\n",
        "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                processed_frames_count += 1\n",
        "                width = image.shape[1]\n",
        "                height = image.shape[0]\n",
        "                # [landmarks, info], with info->x_center ,y_center, r, g, b\n",
        "                ldmks = np.zeros((468, 5), dtype=np.float32)\n",
        "                ldmks[:, 0] = -1.0\n",
        "                ldmks[:, 1] = -1.0\n",
        "                magic_ldmks = []\n",
        "                ### face landmarks ###\n",
        "                results = face_mesh.process(image)\n",
        "                if results.multi_face_landmarks:\n",
        "                    face_landmarks = results.multi_face_landmarks[0]\n",
        "                    landmarks = [l for l in face_landmarks.landmark]\n",
        "                    for idx in range(len(landmarks)):\n",
        "                        landmark = landmarks[idx]\n",
        "                        if not ((landmark.HasField('visibility') and landmark.visibility < VISIBILITY_THRESHOLD)\n",
        "                                or (landmark.HasField('presence') and landmark.presence < PRESENCE_THRESHOLD)):\n",
        "                            coords = mp_drawing._normalized_to_pixel_coordinates(\n",
        "                                landmark.x, landmark.y, width, height)\n",
        "                            if coords:\n",
        "                                ldmks[idx, 0] = coords[1]\n",
        "                                ldmks[idx, 1] = coords[0]\n",
        "\n",
        "                    ### skin extraction ###\n",
        "                    cropped_skin_im, full_skin_im = skin_ex.extract_skin(image, ldmks)\n",
        "\n",
        "                    self.cropped_skin_im_shapes[0].append(cropped_skin_im.shape[0])\n",
        "                    self.cropped_skin_im_shapes[1].append(cropped_skin_im.shape[1])\n",
        "\n",
        "                else:\n",
        "                    cropped_skin_im = np.zeros_like(image)\n",
        "                    full_skin_im = np.zeros_like(image)\n",
        "                    self.cropped_skin_im_shapes[0].append(cropped_skin_im.shape[0])\n",
        "                    self.cropped_skin_im_shapes[1].append(cropped_skin_im.shape[1])\n",
        "\n",
        "                ### sig computing ###\n",
        "                for idx in self.ldmks:\n",
        "                    magic_ldmks.append(ldmks[idx])\n",
        "                magic_ldmks = np.array(magic_ldmks, dtype=np.float32)\n",
        "                temp = sig_ext_met(magic_ldmks, full_skin_im, ldmks_regions,\n",
        "                                   np.int32(SignalProcessingParams.RGB_LOW_TH),\n",
        "                                   np.int32(SignalProcessingParams.RGB_HIGH_TH))\n",
        "                sig.append(temp)\n",
        "\n",
        "                # save landmarks coordinates\n",
        "                self.patch_landmarks.append(magic_ldmks[:,0:3])\n",
        "\n",
        "                # visualize patches and skin\n",
        "                if self.visualize_skin == True:\n",
        "                    self.visualize_skin_collection.append(full_skin_im)\n",
        "                if self.visualize_landmarks == True:\n",
        "                    annotated_image = full_skin_im.copy()\n",
        "                    color = np.array([self.font_color[0],\n",
        "                                      self.font_color[1], self.font_color[2]], dtype=np.uint8)\n",
        "                    for idx in self.ldmks:\n",
        "                        cv2.circle(\n",
        "                            annotated_image, (int(ldmks[idx, 1]), int(ldmks[idx, 0])), radius=0, color=self.font_color, thickness=-1)\n",
        "                        if self.visualize_landmarks_number == True:\n",
        "                            cv2.putText(annotated_image, str(idx),\n",
        "                                        (int(ldmks[idx, 1]), int(ldmks[idx, 0])), cv2.FONT_HERSHEY_SIMPLEX, self.font_size,  self.font_color,  1)\n",
        "                    if self.visualize_patch == True:\n",
        "                        if region_type == \"squares\":\n",
        "                            sides = np.array([self.square] * len(magic_ldmks))\n",
        "                            annotated_image = draw_rects(\n",
        "                                annotated_image, np.array(magic_ldmks[:, 1]), np.array(magic_ldmks[:, 0]), sides, sides, color)\n",
        "                        elif region_type == \"rects\":\n",
        "                            annotated_image = draw_rects(\n",
        "                                annotated_image, np.array(magic_ldmks[:, 1]), np.array(magic_ldmks[:, 0]), np.array(self.rects[:, 0]), np.array(self.rects[:, 1]), color)\n",
        "                    self.visualize_landmarks_collection.append(\n",
        "                        annotated_image)\n",
        "                ### loop break ###\n",
        "                if self.tot_frames is not None and self.tot_frames > 0 and processed_frames_count >= self.tot_frames:\n",
        "                    break\n",
        "        sig = np.array(sig, dtype=np.float32)\n",
        "        return np.copy(sig[:, :, 2:])\n",
        "\n",
        "    def get_landmarks(self):\n",
        "        \"\"\"\n",
        "        Returns landmarks ndarray with shape [num_frames, num_estimators, 2-coords] or empty array\n",
        "        \"\"\"\n",
        "        if hasattr(self, 'patch_landmarks'):\n",
        "            return np.array(self.patch_landmarks)\n",
        "        else:\n",
        "            return np.empty(0)\n",
        "\n",
        "    def get_cropped_skin_im_shapes(self):\n",
        "        \"\"\"\n",
        "        Returns cropped skin shapes with shape [height, width, rgb] or empty array\n",
        "        \"\"\"\n",
        "        if hasattr(self, \"cropped_skin_im_shapes\"):\n",
        "            return np.array(self.cropped_skin_im_shapes)\n",
        "        else:\n",
        "            return np.empty((0, 0, 0))\n",
        "\n"
      ],
      "metadata": {
        "id": "Neq5X4kEyz_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pyVHR/utils/cuda_utils"
      ],
      "metadata": {
        "id": "l-1ScjUkzD3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def cuda_info():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"# CUDA devices: \", torch.cuda.device_count())\n",
        "        for e in range(torch.cuda.device_count()):\n",
        "            print(\"# device number \", e, \": \", torch.cuda.get_device_name(e))\n",
        "\n",
        "\n",
        "def select_cuda_device(n):\n",
        "    torch.cuda.device(n)\n",
        "    cuda.select_device(n)"
      ],
      "metadata": {
        "id": "JTTWZvf9zGfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 딥러닝 파이프라인 클래스 선언"
      ],
      "metadata": {
        "id": "YEzj_F3-sH58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "import ast\n",
        "from numpy.lib.arraysetops import isin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from importlib import import_module, util\n",
        "# from pyVHR.datasets.dataset import datasetFactory\n",
        "# from pyVHR.utils.errors import getErrors, printErrors, displayErrors, BVP_windowing\n",
        "# from pyVHR.extraction.sig_processing import *\n",
        "# from pyVHR.extraction.sig_extraction_methods import *\n",
        "# from pyVHR.extraction.skin_extraction_methods import *\n",
        "# from pyVHR.BVP.BVP import *\n",
        "# from pyVHR.BPM.BPM import *\n",
        "# from pyVHR.BVP.methods import *\n",
        "# from pyVHR.BVP.filters import *\n",
        "from inspect import getmembers, isfunction\n",
        "import os.path\n",
        "# from pyVHR.deepRPPG.mtts_can import *\n",
        "# from pyVHR.deepRPPG.hr_cnn import *\n",
        "# from pyVHR.extraction.utils import *"
      ],
      "metadata": {
        "id": "l4QRQ7_DycRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline():\n",
        "    \"\"\"\n",
        "    This class runs the pyVHR pipeline on a single video or dataset\n",
        "    \"\"\"\n",
        "\n",
        "    minHz = 0.65 # min heart frequency in Hz\n",
        "    maxHz = 4.0  # max heart frequency in Hz\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def run_on_video_multimethods(self, videoFileName,\n",
        "                    winsize,\n",
        "                    ldmks_list=None,\n",
        "                    cuda=True,\n",
        "                    roi_method='convexhull',\n",
        "                    roi_approach='holistic',\n",
        "                    methods=['cpu_CHROM, cpu_POS, cpu_LGI'],\n",
        "                    estimate='holistic',\n",
        "                    movement_thrs=[10, 5, 2],\n",
        "                    patch_size=30,\n",
        "                    RGB_LOW_HIGH_TH = (75,230),\n",
        "                    Skin_LOW_HIGH_TH = (75, 230),\n",
        "                    pre_filt=False,\n",
        "                    post_filt=True,\n",
        "                    verb=True):\n",
        "        \"\"\"\n",
        "        Runs the pipeline on a specific video file.\n",
        "\n",
        "        Args:\n",
        "            videoFileName:\n",
        "                - The video filenane to analyse\n",
        "            winsize:\n",
        "                - The size of the window in frame\n",
        "            ldmks_list:\n",
        "                - (default None) a list of MediaPipe's landmarks to use, the range is: [0:467]\n",
        "            cuda:\n",
        "                - True - Enable computations on GPU\n",
        "            roi_method:\n",
        "                - 'convexhull', uses MediaPipe's lanmarks to compute the convex hull on the face skin\n",
        "                - 'faceparsing', uses BiseNet to parse face components and segment the skin\n",
        "            roi_approach:\n",
        "                - 'holistic', uses the holistic approach, i.e. the whole face skin\n",
        "                - 'patches', uses multiple patches as Regions of Interest\n",
        "            methods:\n",
        "                - A collection of rPPG methods defined in pyVHR\n",
        "            estimate:\n",
        "                - if patches: 'medians', 'clustering', the method for BPM estimate on each window\n",
        "            movement_thrs:\n",
        "                - Thresholds for movements filtering (eg.:[10, 5, 2])\n",
        "            patch_size:\n",
        "                - the size of the square patch, in pixels\n",
        "            RGB_LOW_HIGH_TH:\n",
        "                - default (75,230), thresholds for RGB channels\n",
        "            Skin_LOW_HIGH_TH:\n",
        "                - default (75,230), thresholds for skin pixel values\n",
        "            pre_filt:\n",
        "                - True, uses bandpass filter on the windowed RGB signal\n",
        "            post_filt:\n",
        "                - True, uses bandpass filter on the estimated BVP signal\n",
        "            verb:\n",
        "                - True, shows the main steps\n",
        "        \"\"\"\n",
        "\n",
        "        # set landmark list\n",
        "        if not ldmks_list:\n",
        "            ldmks_list = [2, 3, 4, 5, 6, 8, 9, 10, 18, 21, 32, 35, 36, 43, 46, 47, 48, 50, 54, 58, 67, 68, 69, 71, 92, 93, 101, 103, 104, 108, 109, 116, 117, 118, 123, 132, 134, 135, 138, 139, 142, 148, 149, 150, 151, 152, 182, 187, 188, 193, 197, 201, 205, 206, 207, 210, 211, 212, 216, 234, 248, 251, 262, 265, 266, 273, 277, 278, 280, 284, 288, 297, 299, 322, 323, 330, 332, 333, 337, 338, 345, 346, 361, 363, 364, 367, 368, 371, 377, 379, 411, 412, 417, 421, 425, 426, 427, 430, 432, 436]\n",
        "\n",
        "        # test video filename\n",
        "        assert os.path.isfile(videoFileName), \"The video file does not exists!\"\n",
        "\n",
        "        sig_processing = SignalProcessing()\n",
        "        av_meths = getmembers(pyVHR.BVP.methods, isfunction)\n",
        "        available_methods = [am[0] for am in av_meths]\n",
        "\n",
        "        for m in methods:\n",
        "            assert m in available_methods, \"\\nrPPG method not recognized!!\"\n",
        "\n",
        "        if cuda:\n",
        "            sig_processing.display_cuda_device()\n",
        "            sig_processing.choose_cuda_device(0)\n",
        "\n",
        "        ## 1. set skin extractor\n",
        "        target_device = 'GPU' if cuda else 'CPU'\n",
        "        if roi_method == 'convexhull':\n",
        "            sig_processing.set_skin_extractor(SkinExtractionConvexHull(target_device))\n",
        "        elif roi_method == 'faceparsing':\n",
        "            sig_processing.set_skin_extractor(SkinExtractionFaceParsing(target_device))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown 'roi_method'\")\n",
        "\n",
        "        ## 2. set patches\n",
        "        if roi_approach == 'patches':\n",
        "            sig_processing.set_landmarks(ldmks_list)\n",
        "            sig_processing.set_square_patches_side(np.float(patch_size))\n",
        "\n",
        "        # set sig-processing and skin-processing params\n",
        "        SignalProcessingParams.RGB_LOW_TH = RGB_LOW_HIGH_TH[0]\n",
        "        SignalProcessingParams.RGB_HIGH_TH = RGB_LOW_HIGH_TH[1]\n",
        "        SkinProcessingParams.RGB_LOW_TH = Skin_LOW_HIGH_TH[0]\n",
        "        SkinProcessingParams.RGB_HIGH_TH = Skin_LOW_HIGH_TH[1]\n",
        "\n",
        "        if verb:\n",
        "            print('\\nProcessing Video ' + videoFileName)\n",
        "        fps = get_fps(videoFileName)\n",
        "        sig_processing.set_total_frames(0)\n",
        "\n",
        "        ## 3. ROI selection\n",
        "        if verb:\n",
        "            print('\\nRoi processing...')\n",
        "        sig = []\n",
        "        if roi_approach == 'holistic':\n",
        "            # SIG extraction with holistic\n",
        "            sig = sig_processing.extract_holistic(videoFileName)\n",
        "        elif roi_approach == 'patches':\n",
        "            # SIG extraction with patches\n",
        "            sig = sig_processing.extract_patches(videoFileName, 'squares', 'mean')\n",
        "        if verb:\n",
        "            print(' - Extraction approach: ' + roi_approach)\n",
        "\n",
        "        ## 4. sig windowing\n",
        "        windowed_sig, timesES = sig_windowing(sig, winsize, 1, fps)\n",
        "        if verb:\n",
        "            print(f' - Number of windows: {len(windowed_sig)}')\n",
        "            print(' - Win size: (#ROI, #landmarks, #frames) = ', windowed_sig[0].shape)\n",
        "\n",
        "\n",
        "        ## 5. PRE FILTERING\n",
        "        if verb:\n",
        "            print('\\nPre filtering...')\n",
        "        filtered_windowed_sig = windowed_sig\n",
        "\n",
        "        # -- color threshold - applied only with patches\n",
        "        #if roi_approach == 'patches':\n",
        "        #    filtered_windowed_sig = apply_filter(windowed_sig,\n",
        "        #                                        rgb_filter_th,\n",
        "        #                                        params={'RGB_LOW_TH': RGB_LOW_HIGH_TH[0],\n",
        "        #                                                'RGB_HIGH_TH': RGB_LOW_HIGH_TH[1]})\n",
        "\n",
        "        if pre_filt:\n",
        "            module = import_module('pyVHR.BVP.filters')\n",
        "            method_to_call = getattr(module, 'BPfilter')\n",
        "            filtered_windowed_sig = apply_filter(filtered_windowed_sig,\n",
        "                                                    method_to_call,\n",
        "                                                    fps=fps,\n",
        "                                                    params={'minHz':Pipeline.minHz,\n",
        "                                                            'maxHz':Pipeline.maxHz,\n",
        "                                                            'fps':'adaptive',\n",
        "                                                            'order':6})\n",
        "        if verb:\n",
        "            print(f' - Pre-filter applied: {method_to_call.__name__}')\n",
        "\n",
        "        ## 6. BVP extraction multimethods\n",
        "        bvps_win = []\n",
        "        for method in methods:\n",
        "            if verb:\n",
        "                print(\"\\nBVP extraction...\")\n",
        "                print(\" - Extraction method: \" + method)\n",
        "            module = import_module('pyVHR.BVP.methods')\n",
        "            method_to_call = getattr(module, method)\n",
        "\n",
        "            if 'cpu' in method:\n",
        "                method_device = 'cpu'\n",
        "            elif 'torch' in method:\n",
        "                method_device = 'torch'\n",
        "            elif 'cupy' in method:\n",
        "                method_device = 'cuda'\n",
        "\n",
        "            if 'POS' in method:\n",
        "                pars = {'fps':'adaptive'}\n",
        "            elif 'PCA' in method or 'ICA' in method:\n",
        "                pars = {'component': 'all_comp'}\n",
        "            else:\n",
        "                pars = {}\n",
        "\n",
        "            bvps_win_m = RGB_sig_to_BVP(filtered_windowed_sig,\n",
        "                                    fps, device_type=method_device,\n",
        "                                    method=method_to_call, params=pars)\n",
        "\n",
        "            ## 7. POST FILTERING\n",
        "            if post_filt:\n",
        "                module = import_module('pyVHR.BVP.filters')\n",
        "                method_to_call = getattr(module, 'BPfilter')\n",
        "                bvps_win_m = apply_filter(bvps_win_m,\n",
        "                                    method_to_call,\n",
        "                                    fps=fps,\n",
        "                                    params={'minHz':Pipeline.minHz, 'maxHz':Pipeline.maxHz, 'fps':'adaptive', 'order':6})\n",
        "            if verb:\n",
        "                print(f' - Post-filter applied: {method_to_call.__name__}')\n",
        "            # collect\n",
        "            if len(bvps_win) == 0:\n",
        "                bvps_win = bvps_win_m\n",
        "            else:\n",
        "                for i in range(len(bvps_win_m)):\n",
        "                    bvps_win[i] = np.concatenate((bvps_win[i], bvps_win_m[i]))\n",
        "                    if i == 0: print(bvps_win[i].shape)\n",
        "\n",
        "\n",
        "        ## 8. BPM extraction\n",
        "        if verb:\n",
        "            print(\"\\nBPM estimation...\")\n",
        "            print(f\" - roi appproach: {roi_approach}\")\n",
        "\n",
        "        if roi_approach == 'holistic':\n",
        "            if cuda:\n",
        "                bpmES = BVP_to_BPM_cuda(bvps_win, fps, minHz=Pipeline.minHz, maxHz=Pipeline.maxHz)\n",
        "            else:\n",
        "                bpmES = BVP_to_BPM(bvps_win, fps, minHz=Pipeline.minHz, maxHz=Pipeline.maxHz)\n",
        "\n",
        "        elif roi_approach == 'patches':\n",
        "            if estimate == 'clustering':\n",
        "                #if cuda and False:\n",
        "                #    bpmES = BVP_to_BPM_PSD_clustering_cuda(bvps_win, fps, minHz=Pipeline.minHz, maxHz=Pipeline.maxHz)\n",
        "                #else:\n",
        "                #bpmES = BPM_clustering(sig_processing, bvps_win, winsize, movement_thrs=[15, 15, 15], fps=fps, opt_factor=0.5)\n",
        "                ma = MotionAnalysis(sig_processing, winsize, fps)\n",
        "                bpmES = BPM_clustering(ma, bvps_win, fps, winsize, movement_thrs=movement_thrs, opt_factor=0.5)\n",
        "\n",
        "\n",
        "\n",
        "            elif estimate == 'median':\n",
        "                if cuda:\n",
        "                    bpmES = BVP_to_BPM_cuda(bvps_win, fps, minHz=Pipeline.minHz, maxHz=Pipeline.maxHz)\n",
        "                else:\n",
        "                    bpmES = BVP_to_BPM(bvps_win, fps, minHz=Pipeline.minHz, maxHz=Pipeline.maxHz)\n",
        "                bpmES,_ = BPM_median(bpmES)\n",
        "            if verb:\n",
        "                print(f\" - BPM estimation with: {estimate}\")\n",
        "        else:\n",
        "            raise ValueError(\"Estimation approach unknown!\")\n",
        "\n",
        "        if verb:\n",
        "            print('\\n...done!\\n')\n",
        "\n",
        "        return bvps_win, timesES, bpmES"
      ],
      "metadata": {
        "id": "cBZItzj3sQ_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-kXQ-9irlJO"
      },
      "outputs": [],
      "source": [
        "class DeepPipeline(Pipeline):\n",
        "    \"\"\"\n",
        "    This class runs the pyVHR Deep pipeline on a single video or dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def run_on_video(self, videoFileName, cuda=True, method='MTTS_CAN', bpm_type='welch', post_filt=False, verb=True, crop_face=False):\n",
        "        \"\"\"\n",
        "        Runs the pipeline on a specific video file.\n",
        "\n",
        "        Args:\n",
        "            videoFileName:\n",
        "                - The path to the video file to analyse\n",
        "            cuda:\n",
        "                - True - Enable computations on GPU\n",
        "                - False - Use CPU only\n",
        "            method:\n",
        "                - One of the rPPG methods defined in pyVHR\n",
        "            bpm_type:\n",
        "                - the method for computing the BPM estimate on a time window\n",
        "            post_filt:\n",
        "                - True - Use Band pass filtering on the estimated BVP signal\n",
        "                - False - No post-filtering\n",
        "            verb:\n",
        "               - False - not verbose\n",
        "               - True - show the main steps\n",
        "        \"\"\"\n",
        "\n",
        "        if verb:\n",
        "            print('\\nProcessing Video: ' + videoFileName)\n",
        "        fps = get_fps(videoFileName)\n",
        "        wsize = 6\n",
        "\n",
        "        sp = SignalProcessing()\n",
        "        frames = sp.extract_raw(videoFileName)\n",
        "        print('Frames shape:', frames.shape)\n",
        "\n",
        "        # -- BVP extraction\n",
        "        if verb:\n",
        "            print(\"\\nBVP extraction with method: %s\" % (method))\n",
        "        if method == 'MTTS_CAN':\n",
        "            bvps_pred = MTTS_CAN_deep(frames, fps, verb=1, filter_pred=True)\n",
        "            bvps, timesES = BVP_windowing(bvps_pred, wsize, fps, stride=1)\n",
        "        elif method == 'HR_CNN':\n",
        "            bvps_pred = HR_CNN_bvp_pred(frames)\n",
        "            bvps, timesES = BVP_windowing(bvps_pred, wsize, fps, stride=1)\n",
        "        else:\n",
        "            print(\"Deep Method unsupported!\")\n",
        "            return\n",
        "\n",
        "        if post_filt:\n",
        "            module = import_module('pyVHR.BVP.filters')\n",
        "            method_to_call = getattr(module, 'BPfilter')\n",
        "            bvps = apply_filter(bvps,\n",
        "                                method_to_call,\n",
        "                                fps=fps,\n",
        "                                params={'minHz':0.65, 'maxHz':4.0, 'fps':'adaptive', 'order':6})\n",
        "\n",
        "        # -- BPM extraction\n",
        "        if verb:\n",
        "            print(\"\\nBPM estimation with: %s\" % (bpm_type))\n",
        "        if bpm_type == 'welch':\n",
        "            if cuda:\n",
        "                bpmES = BVP_to_BPM_cuda(bvps, fps, minHz=minHz, maxHz=maxHz)\n",
        "            else:\n",
        "                bpmES = BVP_to_BPM(bvps, fps, minHz=minHz, maxHz=maxHz)\n",
        "        else:\n",
        "            raise ValueError(\"The only 'bpm_type' supported for deep models is 'welch'\")\n",
        "\n",
        "        # median BPM from multiple estimators BPM\n",
        "        median_bpmES, mad_bpmES = BPM_median(bpmES)\n",
        "\n",
        "        if verb:\n",
        "            print('\\n...done!\\n')\n",
        "\n",
        "        return timesES, median_bpmES\n",
        "\n",
        "    def run_on_dataset(self, configFilename, verb=True):\n",
        "        \"\"\"\n",
        "        Runs the tests as specified in the loaded config file.\n",
        "\n",
        "        Args:\n",
        "            configFilename:\n",
        "                - The path to the configuration file\n",
        "            verb:\n",
        "                - False - not verbose\n",
        "                - True - show the main steps\n",
        "\n",
        "               (use also combinations)\n",
        "        \"\"\"\n",
        "        self.configFilename = configFilename\n",
        "        self.parse_cfg(self.configFilename)\n",
        "        # -- cfg parser\n",
        "        parser = configparser.ConfigParser(\n",
        "            inline_comment_prefixes=('#', ';'))\n",
        "        parser.optionxform = str\n",
        "        if not parser.read(self.configFilename):\n",
        "            raise FileNotFoundError(self.configFilename)\n",
        "\n",
        "        # -- verbose prints\n",
        "        if verb:\n",
        "            self.__verbose('a')\n",
        "\n",
        "        # -- dataset & cfg params\n",
        "        if 'path' in self.datasetdict and self.datasetdict['path'] != 'None':\n",
        "            dataset = datasetFactory(\n",
        "                self.datasetdict['dataset'], videodataDIR=self.datasetdict['videodataDIR'], BVPdataDIR=self.datasetdict['BVPdataDIR'], path=self.datasetdict['path'])\n",
        "        else:\n",
        "            dataset = datasetFactory(\n",
        "                self.datasetdict['dataset'], videodataDIR=self.datasetdict['videodataDIR'], BVPdataDIR=self.datasetdict['BVPdataDIR'])\n",
        "\n",
        "        # -- catch data (object)\n",
        "        res = TestResult()\n",
        "\n",
        "        # load all the videos\n",
        "        if self.videoIdx == []:\n",
        "            self.videoIdx = [int(v)\n",
        "                             for v in range(len(dataset.videoFilenames))]\n",
        "\n",
        "        # -- loop on videos\n",
        "        for v in self.videoIdx:\n",
        "            # multi-method -> list []\n",
        "\n",
        "            # -- verbose prints\n",
        "            if verb:\n",
        "                print(\"\\n## videoID: %d\" % (v))\n",
        "\n",
        "            # -- ground-truth signal\n",
        "            try:\n",
        "                fname = dataset.getSigFilename(v)\n",
        "                sigGT = dataset.readSigfile(fname)\n",
        "            except:\n",
        "                continue\n",
        "            winSizeGT = int(self.bvpdict['winSize'])\n",
        "            bpmGT, timesGT = sigGT.getBPM(winSizeGT)\n",
        "\n",
        "            # -- video file name\n",
        "            videoFileName = dataset.getVideoFilename(v)\n",
        "            print(videoFileName)\n",
        "            fps = get_fps(videoFileName)\n",
        "\n",
        "            sp = SignalProcessing()\n",
        "            frames = sp.extract_raw(videoFileName)\n",
        "\n",
        "            # -- loop on methods\n",
        "            for m in self.methods:\n",
        "                if verb:\n",
        "                    print(\"## method: %s\" % (str(m)))\n",
        "\n",
        "                # -- BVP extraction\n",
        "                if str(m) == 'MTTS_CAN':\n",
        "                    bvps_pred = MTTS_CAN_deep(frames, fps, verb=1, filter_pred=True)\n",
        "                    bvps, timesES = BVP_windowing(bvps_pred, winSizeGT, fps, stride=1)\n",
        "                elif str(m) == 'HR_CNN':\n",
        "                    bvps_pred = HR_CNN_bvp_pred(frames)\n",
        "                    bvps, timesES = BVP_windowing(bvps_pred, wsize, fps, stride=1)\n",
        "                else:\n",
        "                    print(\"Deep Method unsupported!\")\n",
        "                    return\n",
        "\n",
        "                # POST FILTERING\n",
        "                postfilter_list = ast.literal_eval(\n",
        "                    self.bvpdict['post_filtering'])\n",
        "                if len(postfilter_list) > 0:\n",
        "                    for f in postfilter_list:\n",
        "                        if verb:\n",
        "                            print(\"  post-filter: %s\" % f)\n",
        "                        fdict = dict(parser[f].items())\n",
        "                        if fdict['path'] != 'None':\n",
        "                            # custom path\n",
        "                            spec = util.spec_from_file_location(\n",
        "                                fdict['name'], fdict['path'])\n",
        "                            mod = util.module_from_spec(spec)\n",
        "                            spec.loader.exec_module(mod)\n",
        "                            method_to_call = getattr(\n",
        "                                mod, fdict['name'])\n",
        "                        else:\n",
        "                            # package path\n",
        "                            module = import_module(\n",
        "                                'pyVHR.BVP.filters')\n",
        "                            method_to_call = getattr(\n",
        "                                module, fdict['name'])\n",
        "                        bvps_win = apply_filter(bvps_win, method_to_call, fps=fps, params=ast.literal_eval(fdict['params']))\n",
        "\n",
        "                # -- BPM extraction\n",
        "                if self.bpmdict['type'] == 'welch':\n",
        "                    bpmES = BVP_to_BPM_cuda(bvps_win, fps, minHz=float(\n",
        "                        self.bpmdict['minHz']), maxHz=float(self.bpmdict['maxHz']))\n",
        "                elif self.bpmdict['type'] == 'clustering':\n",
        "                    bpmES = BVP_to_BPM_PSD_clustering_cuda(bvps_win, fps, minHz=float(\n",
        "                        self.bpmdict['minHz']), maxHz=float(self.bpmdict['maxHz']))\n",
        "\n",
        "                # median BPM from multiple estimators BPM\n",
        "                median_bpmES, mad_bpmES = BPM_median(bpmES)\n",
        "\n",
        "                # -- error metrics\n",
        "                RMSE, MAE, MAX, PCC, CCC, SNR = getErrors(bvps_win, fps, median_bpmES, bpmGT, timesES, timesGT)\n",
        "\n",
        "                # -- save results\n",
        "                res.newDataSerie()\n",
        "                res.addData('dataset', str(self.datasetdict['dataset']))\n",
        "                res.addData('method', str(m))\n",
        "                res.addData('videoIdx', v)\n",
        "                res.addData('RMSE', RMSE)\n",
        "                res.addData('MAE', MAE)\n",
        "                res.addData('MAX', MAX)\n",
        "                res.addData('PCC', PCC)\n",
        "                res.addData('CCC', CCC)\n",
        "                res.addData('SNR', SNR)\n",
        "                res.addData('bpmGT', bpmGT)\n",
        "                res.addData('bpmES', median_bpmES)\n",
        "                res.addData('bpmES_mad', mad_bpmES)\n",
        "                res.addData('timeGT', timesGT)\n",
        "                res.addData('timeES', timesES)\n",
        "                res.addData('videoFilename', videoFileName)\n",
        "                res.addDataSerie()\n",
        "\n",
        "                if verb:\n",
        "                    printErrors(RMSE, MAE, MAX, PCC, CCC, SNR)\n",
        "        return res\n",
        "\n",
        "    def parse_cfg(self, configFilename):\n",
        "        \"\"\" parses the given configuration file for loading the test's parameters.\n",
        "\n",
        "        Args:\n",
        "            configFilename: configuation file (.cfg) name of path .\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.parser = configparser.ConfigParser(\n",
        "            inline_comment_prefixes=('#', ';'))\n",
        "        self.parser.optionxform = str\n",
        "        if not self.parser.read(configFilename):\n",
        "            raise FileNotFoundError(configFilename)\n",
        "\n",
        "        # load paramas\n",
        "        self.datasetdict = dict(self.parser['DATASET'].items())\n",
        "        self.bvpdict = dict(self.parser['BVP'].items())\n",
        "        self.bpmdict = dict(self.parser['BPM'].items())\n",
        "\n",
        "        # video idx list extraction\n",
        "        if isinstance(ast.literal_eval(self.datasetdict['videoIdx']), list):\n",
        "            self.videoIdx = [int(v) for v in ast.literal_eval(\n",
        "                self.datasetdict['videoIdx'])]\n",
        "\n",
        "        # method list extraction\n",
        "        if isinstance(ast.literal_eval(self.bvpdict['methods']), list):\n",
        "            self.methods = ast.literal_eval(\n",
        "                    self.bvpdict['methods'])\n",
        "\n",
        "    def __merge(self, dict1, dict2):\n",
        "        for key in dict2:\n",
        "            if key not in dict1:\n",
        "                dict1[key] = dict2[key]\n",
        "\n",
        "    def __verbose(self, verb):\n",
        "        if verb == 'a':\n",
        "            print(\"** Run the test with the following config:\")\n",
        "            print(\"      dataset: \" + self.datasetdict['dataset'].upper())\n",
        "            print(\"      methods: \" + str(self.methods))"
      ]
    }
  ]
}