{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/B4oiMHs+K3sM3vPXsGR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aytekin827/TIL/blob/main/%EC%A3%BC%EC%8B%9D%EC%98%88%EC%B8%A1%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **주식예측모델**\n",
        "---"
      ],
      "metadata": {
        "id": "TKF9_xtiF7nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 : https://github.com/Se-Hun/StockPrediction"
      ],
      "metadata": {
        "id": "hgxyey-lGCh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. 주식시세 크롤러**\n"
      ],
      "metadata": {
        "id": "zRnT6DTXFT-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tf5GSZ-bESKu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_url(code):\n",
        "    url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code) # url\n",
        "\n",
        "    return url\n",
        "\n",
        "get_url('086520') # 에코프로 코드"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fNrehX1UETkY",
        "outputId": "adb1f931-0c5e-41a4-bb56-eac02084ceb4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://finance.naver.com/item/sise_day.nhn?code=086520'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(dataset):\n",
        "    # Remove NaN Values\n",
        "    dataset = dataset.dropna()\n",
        "\n",
        "    dataset = dataset.rename(columns={'날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open', '고가': 'high', '저가': 'low', '거래량': 'volume'})\n",
        "\n",
        "    # Retype Float -> Int\n",
        "    dataset[['close', 'diff', 'open', 'high', 'low', 'volume']] = \\\n",
        "        dataset[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int)\n",
        "\n",
        "    # Remove \"diff\", \"high\", \"low\"\n",
        "    dataset.drop(['diff', 'high', 'low'], axis='columns', inplace=True)\n",
        "\n",
        "    # Remove Redundancy Data\n",
        "    dataset = dataset.drop_duplicates(['date'], keep='last')\n",
        "\n",
        "    # Retype String -> Datetime\n",
        "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "\n",
        "    # Sort in ascending order by Date\n",
        "    dataset = dataset.sort_values(by=['date'], ascending=True)\n",
        "\n",
        "    # Sort Index For Row\n",
        "    dataset.index = [ i for i in range(len(dataset.index))]\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "stv1Od4UEVEd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_save(output_dir, filename, dataset):\n",
        "    path = os.path.join(output_dir, filename)\n",
        "\n",
        "    dataset.to_csv(path, index=False)"
      ],
      "metadata": {
        "id": "_5Vf0TAaEZva"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_time():\n",
        "    return random.randrange(3, 4) + random.random()"
      ],
      "metadata": {
        "id": "xuqeuTA8Eaig"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--code\", default=\"005930\", type=str, required=True,\n",
        "                        help=\"target company code for crawling\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "    if os.path.exists(\"./StockDataSet\"):\n",
        "        output_dir = \"./StockDataSet\"\n",
        "    else:\n",
        "        os.mkdir(\"./StockDataSet\")\n",
        "        output_dir = \"./StockDataSet\"\n",
        "\n",
        "    code = args.code\n",
        "    # code = '091990'  # code for 셀트리온헬스케어\n",
        "    # code = '005930'  # code for Samsung Electronics\n",
        "    target_bundle_num = 20 #4 # 20\n",
        "    # total data = target_bundle_num * 20 * 10\n",
        "    # 현재 target_bundle_num = 20이므로 total_data = 4000개 -> 4000일 정도의 데이터가 만들어짐(약, 11년 정도?)\n",
        "\n",
        "    dataset = pd.DataFrame()\n",
        "\n",
        "    url = get_url(code)\n",
        "\n",
        "    print(\"***************** Start Crawling *****************\")\n",
        "\n",
        "    count = 0\n",
        "    while count < target_bundle_num:\n",
        "        # 1페이지부터 20페이지를 한 묶음(bundle)으로 봄\n",
        "        start_page = count * 20 + 1  # 1 page 부터\n",
        "        end_page = count * 20 + 20  # 20 page 까지\n",
        "\n",
        "        for page in range(start_page, end_page+1):\n",
        "            print(\"Current Page : {}\".format(page))\n",
        "\n",
        "            time.sleep(random_time()) # => Naver가 Crawling을 막아서 요청을 조금 느리게 해야함..\n",
        "            page_url = '{url}&page={page}'.format(url=url, page=page)\n",
        "\n",
        "            dataset = dataset.append(pd.read_html(page_url, header=0)[0], ignore_index=True)\n",
        "\n",
        "        count = count + 1\n",
        "        time.sleep(random_time()) # => Naver가 Crawling을 막아서 요청을 조금 느리게 해야함..\n",
        "\n",
        "    # preprocessing dataset\n",
        "    dataset = preprocess_dataset(dataset)\n",
        "\n",
        "    start_date = dataset['date'][0]\n",
        "    start_date = str(start_date).split()[0]\n",
        "    end_date = dataset['date'][len(dataset.index) - 1]\n",
        "    end_date = str(end_date).split()[0]\n",
        "\n",
        "    # DataFrame -> CSV File\n",
        "    filename = \"data_\" + str(start_date) + \"_\" + str(end_date) + \".csv\"\n",
        "    data_save(output_dir, filename, dataset)\n",
        "\n",
        "    print(\"***************** Result Report *****************\")\n",
        "\n",
        "    print(\"Start Date : {}\".format(start_date))\n",
        "    print(\"End Date : {}\".format(end_date))\n",
        "\n",
        "    print(\"***************** End Crawling *****************\")\n",
        "\n",
        "    # print(dataset.head())\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "UI7TgL62EYlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. 모델링**"
      ],
      "metadata": {
        "id": "HbJS_wFyGHhv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYnTT4PEQMs"
      },
      "outputs": [],
      "source": []
    }
  ]
}